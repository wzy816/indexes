@startuml
skinparam RectangleBackgroundColor black
skinparam RectangleBorderColor white
skinparam RectangleFontColor white
skinparam InterfaceBackgroundColor black
skinparam InterfaceBorderColor white
skinparam InterfaceFontColor white
skinparam FileBackgroundColor black
skinparam FileBorderColor white
skinparam FileFontColor white
skinparam FolderBackgroundColor black
skinparam FolderBorderColor white
skinparam FolderFontColor white
skinparam DatabaseBackgroundColor black 
skinparam DatabaseBorderColor white
skinparam DatabaseFontColor white
skinparam CloudBackgroundColor black
skinparam CloudBorderColor white
skinparam CloudFontColor White
skinparam StorageBackgroundColor #acacac
skinparam StorageBorderColor #acacac
skinparam FrameBackgroundColor #efefef
skinparam FrameBorderColor black
skinparam ComponentBackgroundColor #efefef 
skinparam ComponentBorderColor black
skinparam ArrowColor gray
skinparam ArrowFontColor gray
skinparam ActorBorderColor black
skinparam ActorBackgroundColor white
skinparam ParticipantBorderColor black
skinparam ParticipantBackgroundColor white
skinparam HeaderFontColor black
skinparam LegendBackgroundColor white
skinparam LegendBorderColor white
skinparam DefaultFontName Blue Sky Standard
skinparam DefaultFontSize 12
skinparam shadowing false
skinparam defaultTextAlignment centery

set namespaceSeparator ::

package OmniAnomaly {
    package main {
        class ExpConfig <<(F,#00bebe)>>
        class main <<(F,#00bebe)>> {
            ... = get_data
            ..
            model = OmniAnomaly
            ..
            trainer = Trainer
            ..
            predictor = Predictor
            ..
            ...
            trainer.fit with TrainLoop as context
        }
        note right: main entry
        ExpConfig --> main
    }

    package model {
        class OmniAnomaly {
            .. init() ..
            ...
            self._vae = VAE
            ...
            ..
            tf.Tensor get_training_loss()
            tf.Tensor get_score()
        }
        note right: L30 build VAE
    }

    package training {
        class Trainer {
            .. init() ..
            loss = model.get_training_loss L120
            ...
            .. fit() ..
            用了 BatchSlidingWindow
            loop
            session.run
            退火 learning rate
        }
    }

    package vae{
        class VAE {
            .. variational() ..
            return BayesianNet encoder
            x -> h_for_q_z(rnn) -> q_z_given_x(RecurrentDistribution) -> z -> flow
            return StochasticTensor
            .. model() ..
            return BayesianNet decoder
            p_z (ssm) -> h_for_p_x -> p_x_given_z
            .. chain() ..
            build variational encoder
            build model decoder
            pass variational and model to build VariationalChain
            .. get_training_loss() ..
            call vae.chain
            cal x_log_prob
            use sgvb
        }
        note right:  build network, get_training_loss computes obj
    }

    package prediction {
        class Predictor {
            np.ndarray get_score() 计算recon prob
        }
        note right:  L180 compute recon prob
    }
    
    package wrapper {
        class TfpDistribution {
            .. sample() ..
            .. log_prob() ..
        }
        class softplus_std <<(F,#54585a)>> {
            wrapper of tf.layers.dense + softplus
        }
        note right: std layer = dense+softplus
        class wrap_params_net <<(F,#54585a)>> {
            build h_for_hist
            then mean_layer (tf.layers.dense) 和 
            std_layer (tf.layers.dense + softplus)
        }
        class rnn <<(F,#54585a)>> {
            unstack input along time axis
            use rnn.static_rnn build upon fw_cell
            stack output along time axis
            add 2 dense layers to output shape
            (batch_size,win_length,rnn_num_hidden=500)
        }
        note right: L114 rnn
    }

    package recurrent_distribution {
        class RecurrentDistribution {
            .. __init__() ..
            self.mean_q_mlp 是 tf.layers.dense
            self.std_q_mlp 是 tf.layers.dense + softplus
            .. sample_step() ..
            .. log_prob_step() ..
            .. sample() ..

            .. log_prob() ..
        }
        note right: diff from Normal sample L88 sample
    }
}


package tfsnippet {
    package bayes {
        class BayesianNet {
            use OrderedDict to store stochastic tensor
            .. add() ..
            add StochasticTensor to OrderedDict 
            .. variational_chain() ..
            obviously log_joint is None
            call VariationalChain
            .. local_log_probs() ..
            compute log_prob of all stochastic tensors
        }
    }
    package stochastic {
        class StochasticTensor
    }
    package distributions {
        class Distribution {
            base class
            .. prob() ..
            tf.exp(self.log_prob)
        }
        class Normal {
            ..
            mean()
            logstd()
            std()
        }
        class ZhuSuanDistribution {
            .. init() ..
            use zhusuan.distributions.Distribution as self._distribution
            .. StochasticTensor sample() ..
            first self._distribution.sample
            then t = StochasticTensor
            and compute prob
        }
        class FlowDistribution {

        }
    }
    package variational {
        package chain {
            class VariationalChain {
                .. init() ..
                model.local_log_probs
                compute latent_log_probs
                self._vi = VariationalInference
            }
            note right: L51 
        }
        package inference {
            class VariationalInference {
                .. init() ..
                build _lower_bound
                build _training
                build _evaluation 
                .. zs_elbo() ..
                wrap zs.variational.elbo
                .. zs_importance_weighted_objective() ..
                wrap zs.variational.importance_weighted_objective
                .. zs_klpq() ..
                wrap zs.variational.klpq

            }
            class VariationalLowerBounds {
                .. elbo() ..
                .. monte_carlo_objective() ..
            }
            class VariationalTrainingObjectives {
                .. sgvb() ..
                .. reinforce() ..
                .. iwae() ..
                .. vimco() ..
                .. rws_wake() ..
            }
            class VariationalEvaluation {
                .. importance_sampling_log_likelihood() ..
            }
        }
        package evaluation {
            class importance_sampling_log_likelihood <<(F,#54585a)>>
        }
        package objectives {
            class elbo_objective <<(F,#54585a)>>
            note right: L39 equation
            class monte_carlo_objective <<(F,#54585a)>>
        }
        package estimators {
            class sgvb_estimator <<(F,#54585a)>>
            class iwae_estimator <<(F,#54585a)>>
        }
    }
    package layers {
        package flow {
            class PlanarNormalizingFlow {
                compute y and log_det
            }
            class planar_normalizing_flows <<(F,#00bebe)>> {
                add n_layers of PlanarNormalizingFlow in a sequence
            } 
            class SequentialFlow {
            }
        }
    }
}

package tensorflow_probability {
    package python.distributions {
        class LinearGaussianStateSpaceModel {
            forward_filter()
        }
    }
}

package zhusuan {
    class distributions.Normal {
        .. _sample() ..
        use tf.random.normal to sample
        reflect reparameterization
        align n_samples 和 batch_shape
        .. _log_prob() ..
        follow math equations
    }
    note right: L161 sample; L174 log_prob 
} 

TfpDistribution <|-- Distribution
ZhuSuanDistribution <|-- Distribution
Normal <|-- ZhuSuanDistribution

main <.. OmniAnomaly : model instance
main <.. Trainer : trainer instance
main <.. Predictor : predictor instance

Trainer ..> OmniAnomaly : call get_training_loss
Predictor ..> OmniAnomaly : call get_score
OmniAnomaly ..> VAE : build VAE
VAE ..> TfpDistribution : p_z
TfpDistribution ..> LinearGaussianStateSpaceModel : z.log_prob 
VAE ..> Normal : p_x_given_z
VAE ..> RecurrentDistribution :  q_z_given_x
Normal ..> distributions.Normal
VAE ..> wrap_params_net : h_for_p_x
VAE ..> rnn : h_for_q_z
VAE ..> planar_normalizing_flows : posterior_flow
planar_normalizing_flows ..> SequentialFlow
SequentialFlow ..> PlanarNormalizingFlow

VAE ..> BayesianNet : chain() call  variational_chain()
BayesianNet ..> VariationalChain : variational_chain() call new VariationalChain 
BayesianNet ..> FlowDistribution : add() if use flow
VariationalChain ..> BayesianNet : init call both variation and model's local_log_probs
VariationalChain ..> VariationalInference : init call

VariationalInference ..> VariationalLowerBounds : init call
VariationalInference ..> VariationalTrainingObjectives : init call
VariationalInference ..> VariationalEvaluation : init call
VariationalLowerBounds ..> elbo_objective : elbo()
VariationalLowerBounds ..> monte_carlo_objective : monte_carlo_objective()
VariationalTrainingObjectives ..> sgvb_estimator : sgvb()
VariationalTrainingObjectives ..> iwae_estimator : iwae()
VariationalTrainingObjectives ..> VariationalInference : reinforce() call zs_elbo().reinforce
VariationalTrainingObjectives ..> VariationalInference : vimco() call zs_importance_weighted_objective().vimco()
VariationalTrainingObjectives ..> VariationalInference : rws_wake() call zs_klpq().rws()
VariationalEvaluation ..> importance_sampling_log_likelihood 
@enduml