{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0c6185",
   "metadata": {},
   "source": [
    "# rotary position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7748ca73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple/\n",
      "Requirement already satisfied: torch in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (66.0.0)\n",
      "Requirement already satisfied: wheel in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: cmake in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/anaconda3/envs/jupyter/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cf2f8",
   "metadata": {},
   "source": [
    "## 1. math from paper\n",
    "basic equation\n",
    "$$f(q,m)=\n",
    "\\begin{pmatrix}\n",
    " \\cos m\\theta & -\\sin m\\theta  \\\\\n",
    " \\sin m\\theta & \\cos m\\theta\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " q_{0}  \\\\\n",
    " q_{1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "to expand to full matrix\n",
    "\n",
    "$$\n",
    "=\\begin{pmatrix}\n",
    " \\cos m\\theta_{0} & -\\sin m\\theta_{0} & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    " \\sin m\\theta_{0} & \\cos m\\theta_{0} & 0 & 0 & \\cdots & 0 & 0 \\\\\n",
    " 0 & 0 & \\cos m\\theta_{1} & -\\sin m\\theta_{1} & \\cdots & 0 & 0 \\\\\n",
    " 0 & 0 & \\sin m\\theta_{1} & \\cos m\\theta_{1} & \\cdots  & 0 & 0 \\\\\n",
    " \\vdots &  \\vdots &  \\vdots &  \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    " 0 & 0 & 0 & 0 & \\cdots & \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\\\\n",
    " 0 & 0 & 0 & 0 & \\cdots & \\sin m\\theta_{d/2} & \\cos m\\theta_{d/2}\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " q_{0} \\\\\n",
    " q_{1} \\\\\n",
    " q_{2} \\\\\n",
    " q_{3} \\\\\n",
    " \\vdots \\\\\n",
    " q_{d/2-2} \\\\\n",
    " q_{d/2-1} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "with rewrite to be computational efficient\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    " q_{0} \\\\\n",
    " q_{1} \\\\\n",
    " q_{2} \\\\\n",
    " q_{3} \\\\\n",
    " \\vdots \\\\\n",
    " q_{d-2} \\\\\n",
    " q_{d-1} \\\\\n",
    "\\end{pmatrix}\n",
    "\\bigotimes\n",
    "\\begin{pmatrix}\n",
    " cos m\\theta_{0} \\\\\n",
    " cos m\\theta_{0} \\\\\n",
    " cos m\\theta_{1} \\\\\n",
    " cos m\\theta_{1} \\\\\n",
    " \\vdots \\\\\n",
    " cos m\\theta_{d/2-1} \\\\\n",
    " cos m\\theta_{d/2-1} \\\\\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    " -q_{1} \\\\\n",
    " q_{0} \\\\\n",
    " -q_{3} \\\\\n",
    " q_{2} \\\\\n",
    " \\vdots \\\\\n",
    " -q_{d-1} \\\\\n",
    " q_{d-2} \\\\\n",
    "\\end{pmatrix}\n",
    "\\bigotimes\n",
    "\\begin{pmatrix}\n",
    " sin m\\theta_{0} \\\\\n",
    " sin m\\theta_{0} \\\\\n",
    " sin m\\theta_{1} \\\\\n",
    " sin m\\theta_{1} \\\\\n",
    " \\vdots \\\\\n",
    " sin m\\theta_{d/2-1} \\\\\n",
    " sin m\\theta_{d/2-1} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f55dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# config\n",
    "dim = 4096 # also embed_size\n",
    "max_seq_len = 2048 # 最长输入长度\n",
    "bsz = 16 # batch_size\n",
    "seq_len = 6 # 当前输入长度\n",
    "num_heads = 32 #\n",
    "head_dim = dim // num_heads # 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038beae3",
   "metadata": {},
   "source": [
    "## 2. implementation at roformer (rope original paper)\n",
    "\n",
    "cal sinusoidal position\n",
    "\n",
    "https://github.com/JunnYu/RoFormer_pytorch/blob/roformer_v2/src/roformer/modeling_roformer.py#L156\n",
    "\n",
    "apply rotary\n",
    "\n",
    "https://github.com/JunnYu/RoFormer_pytorch/blob/roformer_v2/src/roformer/modeling_roformer.py#L441\n",
    "\n",
    "the code follows math equation by rotating in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea7abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def roformer_apply_rotary(x):\n",
    "    \n",
    "    # RoFormerSinusoidalPositionalEmbedding\n",
    "    position_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / head_dim) for j in range(head_dim)]\n",
    "            for pos in range(seq_len)\n",
    "        ]\n",
    "    )\n",
    "    sin = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "    cos = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "#     sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
    "#     out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "#     out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "    out = torch.cat((sin,cos),dim=-1) # [seq_len,head_dim]\n",
    "    \n",
    "    \n",
    "    # RoFormerEncoder.forward\n",
    "    sinusoidal_pos = out[None,None,:,:].chunk(2, dim=-1) # [1, 1, seq_len, head_dim // 2] \n",
    "    \n",
    "    \n",
    "    # RoFormerSelfAttention.apply_rotary\n",
    "    sin,cos = sinusoidal_pos\n",
    "    x1, x2 = x[..., 0::2], x[..., 1::2]\n",
    "    return torch.stack([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=-1).flatten(-2, -1), cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6f09fd",
   "metadata": {},
   "source": [
    "## 3. implementation at GPTNeo\n",
    "https://github.com/EleutherAI/gpt-neo/blob/master/models/layers.py#L355\n",
    "\n",
    "this is **different** from paper cause it uses rotate_half instead of rotate_every_two\n",
    "\n",
    "confirmed by https://github.com/EleutherAI/gpt-neox/pull/241 and discussed in https://github.com/EleutherAI/gpt-neox/issues/834 that it is efficient without performance loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35364ca0",
   "metadata": {},
   "source": [
    "## 4. implementation at facebook llama\n",
    "\n",
    "llama paper indicates that it is inspired from GPTNeo\n",
    "\n",
    "but the rope use complex number trick that actually follow the original paper, NOT GPTNeo rotate_half\n",
    "\n",
    "https://github.com/facebookresearch/llama/blob/main/llama/model.py#L63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05afc9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_apply_rotary_emb(x):\n",
    "    # L123 llama use x shape (bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "    # so here transpose back\n",
    "    x_ = x.transpose(1, 2) # (bsz, seqlen, num_heads, head_dim)\n",
    "    \n",
    "    # precompute_freqs_cis\n",
    "    theta = 10000\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "    t = torch.arange(max_seq_len*2)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    \n",
    "    \n",
    "    # Transformer.forward after tok_embeddings\n",
    "    freqs_cis = freqs_cis[0:seq_len] # (seq_len, head_dim/2)\n",
    "    \n",
    "    \n",
    "    # apply_rotary_emb\n",
    "    x_ = x_.float().reshape(*x_.shape[:-1], -1, 2) # last dimension [x0,x1,x2,..] to [[x0,x1],[x2,]..] pairs, (bsz, seqlen, num_heads, head_dim/2, 2)\n",
    "    x_ = torch.view_as_complex(x_) # (bsz, seqlen, num_heads, head_dim/2)\n",
    "    \n",
    "    \n",
    "    # reshape_for_broadcast\n",
    "    ndim = x_.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x_.shape[1], x_.shape[-1])\n",
    "    shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_.shape)]\n",
    "    freqs_cis =freqs_cis.view(*shape) # (1, seq_len, 1, head_dim/2)\n",
    "    \n",
    "    \n",
    "    o = torch.view_as_real(x_ * freqs_cis) # (bsz, seq_len, num_heads, head_dim/2, 2)\n",
    "    o = o.flatten(3) # (bsz, seq_len, num_heads, head_dim)\n",
    "    return o.transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c259e",
   "metadata": {},
   "source": [
    "## 5. implementation at transformers Llama\n",
    "\n",
    "https://github.com/fpgaminer/transformers/blob/main/src/transformers/models/llama/modeling_llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bc2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformers_apply_rotary_pos_emb(x):\n",
    "    # LlamaModel.forward\n",
    "    position_ids = torch.arange(0, seq_len) #(1, seq_len)\n",
    "    position_ids = position_ids.unsqueeze(0).view(-1, seq_len) \n",
    "    \n",
    "    \n",
    "    # LlamaRotaryEmbedding\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float()/ head_dim)) # (max_seq_len)\n",
    "    t = torch.arange(max_seq_len, dtype=inv_freq.dtype)\n",
    "    freqs = torch.einsum(\"i,j->ij\", t, inv_freq) # (max_seq_len,max_seq_len)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos_cached = emb.cos()[None, None, :, :]\n",
    "    sin_cached = emb.sin()[None, None, :, :]\n",
    "    cos = cos_cached[:,:,:seq_len,...]\n",
    "    sin = sin_cached[:,:,:seq_len,...]\n",
    "    \n",
    "    \n",
    "    def rotate_half(x):\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    \n",
    "    # apply_rotary_pos_emb\n",
    "    cos = cos.squeeze(1).squeeze(0)  \n",
    "    sin = sin.squeeze(1).squeeze(0)  \n",
    "    cos = cos[position_ids].unsqueeze(1)  # (1, 1, seq_len, head_dim)\n",
    "    sin = sin[position_ids].unsqueeze(1)  # (1, 1, seq_len, head_dim)\n",
    "    \n",
    "    return (x * cos) + (rotate_half(x) * sin), cos, sin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8542f",
   "metadata": {},
   "source": [
    "## 6. test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe6097",
   "metadata": {},
   "source": [
    "prove that\n",
    "- RoPE implementation differenct leads to different embed result\n",
    "- roformer = facebook llama != transformers llama / GPTNeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b970da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn((bsz, seq_len, dim))\n",
    "    q = torch.nn.Linear(dim,dim,bias=False)\n",
    "    query_states = q(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "\n",
    "    e1,cos1,sin1 = roformer_apply_rotary(query_states)\n",
    "    e2 = llama_apply_rotary_emb(query_states)\n",
    "    e3,cos3, sin3 = transformers_apply_rotary_pos_emb(query_states)\n",
    "\n",
    "    # e1 = e2 != e3\n",
    "    print(torch.allclose(e1,e2,atol=1e-5))\n",
    "    print(torch.allclose(e2,e3,atol=1e-4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4b3e0b",
   "metadata": {},
   "source": [
    "prove that\n",
    "- sinusoidal_pos calculation result is same, only differ in shape\n",
    "- cos3 = [cos1,cos1], sin3 = [sin1,sin1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1702760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    print(torch.allclose(cos1,cos3.chunk(2,dim=-1)[0]))\n",
    "    print(torch.allclose(cos1,cos3.chunk(2,dim=-1)[1]))\n",
    "    print(torch.allclose(sin1,sin3.chunk(2,dim=-1)[0]))\n",
    "    print(torch.allclose(sin1,sin3.chunk(2,dim=-1)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29e0f5",
   "metadata": {},
   "source": [
    "prove that \n",
    "\n",
    "- to make sliced rotary work the same as the original, transformers' magic is this permute function during weight conversion https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py#L101\n",
    "- confirmed by https://discuss.huggingface.co/t/why-llama-weight-in-huggingface-need-to-do-permute-on-wq-wk/37643\n",
    "\n",
    "- so using transformers LlamaForCausalLM for inference will works the same as facebook llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9c5208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wq != wq2\n",
      "query_states != query_states2\n",
      "key_states != key_states2\n",
      "eq != eq2\n",
      "ek != ek2\n",
      "scores == scores2\n",
      "output == output2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn((bsz, seq_len, dim))\n",
    "\n",
    "    # use roformer    \n",
    "    wq = torch.nn.Linear(dim,dim,bias=False)\n",
    "    wk = torch.nn.Linear(dim,dim,bias=False)\n",
    "    wv = torch.nn.Linear(dim,dim,bias=False)\n",
    "\n",
    "    query_states = wq(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "    key_states = wk(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "    values = wv(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "  \n",
    "    eq,_,_ = roformer_apply_rotary(query_states)\n",
    "    ek,_,_ = roformer_apply_rotary(key_states)\n",
    "    \n",
    "    scores = torch.matmul(eq, ek.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    scores = torch.nn.functional.softmax(scores.float(), dim=-1)\n",
    "    output = torch.matmul(scores, values)\n",
    "    \n",
    "    \n",
    "    # use transformers Llama\n",
    "    # permute wq and wk\n",
    "    def permute(w):\n",
    "        return w.view(num_heads, head_dim // 2, 2, dim).transpose(1, 2).reshape(dim, dim)\n",
    "    wq2 = torch.nn.Linear(dim,dim,bias=False)\n",
    "    wq2.weight.copy_(permute(wq.weight))\n",
    "    wk2 = torch.nn.Linear(dim,dim,bias=False)\n",
    "    wk2.weight.copy_(permute(wk.weight))\n",
    "\n",
    "    query_states2 = wq2(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "    key_states2 = wk2(x).view(bsz, seq_len, num_heads, head_dim).transpose(1, 2) # (bsz, num_heads, seq_len, head_dim)\n",
    "\n",
    "    \n",
    "    eq2,_,_ = transformers_apply_rotary_pos_emb(query_states2)\n",
    "    ek2,_,_ = transformers_apply_rotary_pos_emb(key_states2) \n",
    "\n",
    "    scores2 = torch.matmul(eq2, ek2.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    scores2 = torch.nn.functional.softmax(scores2, dim=-1)\n",
    "    output2 = torch.matmul(scores2, values)  \n",
    "    \n",
    "    \n",
    "    print('wq {} wq2'.format('==' if torch.allclose(wq.weight,wq2.weight) else '!='))\n",
    "    print('query_states {} query_states2'.format('==' if torch.allclose(query_states,query_states2) else '!='))\n",
    "    print('key_states {} key_states2'.format('==' if torch.allclose(key_states,key_states2) else '!='))   \n",
    "    print('eq {} eq2'.format('==' if torch.allclose(eq,eq2) else '!='))\n",
    "    print('ek {} ek2'.format('==' if torch.allclose(ek,ek2) else '!='))\n",
    "    print('scores {} scores2'.format('==' if torch.allclose(scores,scores2) else '!='))\n",
    "    print('output {} output2'.format('==' if torch.allclose(output,output2,atol=1e-6) else '!=')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2ca23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
