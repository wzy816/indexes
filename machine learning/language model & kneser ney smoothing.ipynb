{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "## Resources\n",
    "\n",
    "textbook https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "Dan Jurafsky series of videos https://www.youtube.com/watch?v=Saq1QagC8KY&list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm&index=13&t=15s\n",
    "\n",
    "## Basic\n",
    "\n",
    "Language Model is a model that assign probabilities to sequences of words, aka LM.\n",
    "\n",
    "Tasks include:\n",
    "- compute the probability of a sentence $P(W)$\n",
    "- compute the probability of a word given several words $P(W_i|W_1W_2 \\dots W_n)$\n",
    "\n",
    "\n",
    "## Chain Rule\n",
    "\n",
    "$P(W_1 W_2 \\dots W_k) = P(W_1) P(W_2|W_1) P(W_3|W_1 W_2) \\dots P(W_k | W_1 W_2 \\dots W_{k-1})$\n",
    "\n",
    "## Markov Assumption\n",
    "\n",
    "The probability of a word depends only on the previous word.\n",
    "\n",
    "$ P(W_i|W_1 \\dots W_i-1) \\approx P(W_i | W_{i-n+1}| \\dots W_{i-1})$\n",
    "\n",
    "## N-gram model\n",
    "\n",
    "### 1. Unigram Model\n",
    "\n",
    "The probability of a sentence is the product of probabilities of all words.\n",
    "\n",
    "Words are independent. \n",
    "\n",
    "$P(w_1 w_2 \\cdots w_n)=\\prod_{i}P(w_i)$\n",
    "\n",
    "### 2. Bigram Model\n",
    "\n",
    "Word depends on its previous one word.\n",
    "\n",
    "$P(w_i | w_1 w_2 \\dots w_{i-1}) \\approx P( w_i |  w_{i-1})$\n",
    "\n",
    "more, tri-gram, 4-gram, 5-gram ...\n",
    "\n",
    "### Flaws\n",
    "\n",
    "- Language has long-distance dependencies.\n",
    "\n",
    "### Note\n",
    "\n",
    "- Use log instead of multiple to avoid underflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimate\n",
    "\n",
    "- bigram form\n",
    "$$\n",
    "P_{MLE}(w_i | w_{i-1}) = \n",
    "\\frac\n",
    "    {C(w_{i-1},w_i)}\n",
    "    {C(w_{i-1})}\n",
    "$$\n",
    "\n",
    "- general form\n",
    "\n",
    "$$\n",
    "P_{MLE}(w_i | w_{i-n+1}^{i-1}) = \n",
    "\\frac\n",
    "    {C(w_{i-n+1}^i)}\n",
    "    {C(w_{i-n+1}^{i-1})}\n",
    "=\n",
    "\\frac\n",
    "    {C(w_{i-n+1}^i)}\n",
    "    {\\sum_{w_i} C(w_{i-n+1}^i)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "w_{i-n+1}^{i-1} \\ denotes \\ word \\ w_{i-n+1}w_{i-n+2}\\dots w_{i-1}\n",
    "$$\n",
    "\n",
    "MLE fails when the n-gram is not in the data. \n",
    "\n",
    "The solution is **Smoothing**.\n",
    "\n",
    "## Smoothing\n",
    "\n",
    "- an example of kn smoothing https://medium.com/@seccon/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8\n",
    "\n",
    "- a kn implementation with bug https://github.com/smilli/kneser-ney and the blog http://smithamilli.com/blog/kneser-ney/\n",
    "\n",
    "- paper gives overview of all smoothing algos http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf\n",
    "\n",
    "- wiki https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\n",
    "\n",
    "### Absolute Discounting Interpolation\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P_{ad}(w_i | w_{i-1}) & = \\frac{c(w_{i-1},w_i)-d}{c(w_{i-1}} + \\lambda(w_{i-1})P(w) \\\\ \n",
    "& = \\frac{discounted \\ bigram}{sth} + interpolation \\ weight * unigram\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where d could be 0.75 from experience\n",
    "\n",
    "### Kneser-Ney Smoothing\n",
    "\n",
    "Change: **P(w)** \"how likely is w\" => **P_continuation(w)** \"how likely is w to appear as a novel continuation\"\n",
    "\n",
    "\n",
    "#### 1. bigram formulation\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-1}) = \\frac{max(c(w_{i-1}, w_i) -d, 0)}{c(w_{i-1})}  + \\lambda(w_{i-1}) P_{continuation}(w_i) \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\lambda(w_{i-1}) & = \\frac{d}{c(w_{i-1})}|\\{w: c(w_{i-1},w) > 0 \\}| \\\\\n",
    "& = the \\ normalized \\ discount * the \\ number \\ of \\ word \\ types \\ that \\ follow \\ w_{i-1} \\\\\n",
    "& = low \\ order \\ weight \\ or \\ backoff \\ weight\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "P_{continuation}(w_i) & = \\frac{|\\{w: c(w,w_{i}) > 0 \\}|}{|\\{(w_{j-1},w_j): c(w_{j-1},w_j) > 0 \\}|} \\\\\n",
    "& = \\frac{number \\ when \\ word \\ w_{i} \\ is \\ a \\ novel \\ continuation}{total \\ number \\ of \\ word \\ bigram \\ types} \\\\\n",
    "& = low \\ order \\ probability\n",
    "\\end{split}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table1Gram \n",
      " {'paragraphs': 2, 'are': 1, 'the': 5, 'building': 1, 'blocks': 1, 'of': 8, 'papers': 1, 'many': 1, 'students': 1, 'define': 1, 'in': 6, 'terms': 1, 'length': 2, 'a': 15, 'paragraph': 8, 'is': 6, 'group': 3, 'at': 1, 'least': 1, 'five': 1, 'sentences': 4, 'half': 1, 'page': 1, 'long': 2, 'etc': 1, 'reality': 1, 'though': 1, 'unity': 1, 'and': 2, 'coherence': 1, 'ideas': 1, 'among': 1, 'what': 2, 'constitutes': 1, 'defined': 1, 'as': 2, 'or': 2, 'single': 1, 'sentence': 3, 'that': 2, 'forms': 1, 'unit': 1, 'appearance': 1, 'do': 1, 'not': 1, 'determine': 1, 'whether': 1, 'section': 1, 'paper': 1, 'for': 1, 'instance': 1, 'some': 1, 'styles': 2, 'writing': 1, 'particularly': 1, 'journalistic': 1, 'can': 1, 'be': 1, 'just': 1, 'one': 2, 'ultimately': 1, 'support': 1, 'main': 1, 'idea': 2, 'this': 2, 'handout': 1, 'we': 1, 'will': 1, 'refer': 1, 'to': 1, 'controlling': 1, 'because': 1, 'it': 1, 'controls': 1, 'happens': 1, 'rest': 1} \n",
      "\n",
      "table2Gram \n",
      " {'paragraphs are': 1, 'are the': 1, 'the building': 1, 'building blocks': 1, 'blocks of': 1, 'of papers': 1, 'many students': 1, 'students define': 1, 'define paragraphs': 1, 'paragraphs in': 1, 'in terms': 1, 'terms of': 1, 'of length': 1, 'length a': 1, 'a paragraph': 7, 'paragraph is': 4, 'is a': 3, 'a group': 2, 'group of': 3, 'of at': 1, 'at least': 1, 'least five': 1, 'five sentences': 1, 'is half': 1, 'half a': 1, 'a page': 1, 'page long': 1, 'in reality': 1, 'the unity': 1, 'unity and': 1, 'and coherence': 1, 'coherence of': 1, 'of ideas': 1, 'ideas among': 1, 'among sentences': 1, 'sentences is': 1, 'is what': 1, 'what constitutes': 1, 'constitutes a': 1, 'is defined': 1, 'defined as': 1, 'as a': 1, 'of sentences': 2, 'sentences or': 1, 'or a': 1, 'a single': 1, 'single sentence': 1, 'sentence that': 1, 'that forms': 1, 'forms a': 1, 'a unit': 1, 'length and': 1, 'and appearance': 1, 'appearance do': 1, 'do not': 1, 'not determine': 1, 'determine whether': 1, 'whether a': 1, 'a section': 1, 'section in': 1, 'in a': 1, 'a paper': 1, 'paper is': 1, 'for instance': 1, 'in some': 1, 'some styles': 1, 'styles of': 1, 'of writing': 1, 'particularly journalistic': 1, 'journalistic styles': 1, 'paragraph can': 1, 'can be': 1, 'be just': 1, 'just one': 1, 'one sentence': 1, 'sentence long': 1, 'a sentence': 1, 'sentence or': 1, 'or group': 1, 'sentences that': 1, 'that support': 1, 'support one': 1, 'one main': 1, 'main idea': 1, 'in this': 1, 'this handout': 1, 'we will': 1, 'will refer': 1, 'refer to': 1, 'to this': 1, 'this as': 1, 'as the': 1, 'the controlling': 1, 'controlling idea': 1, 'because it': 1, 'it controls': 1, 'controls what': 1, 'what happens': 1, 'happens in': 1, 'in the': 1, 'the rest': 1, 'rest of': 1, 'of the': 1, 'the paragraph': 1} \n",
      "\n",
      "table3Gram \n",
      " {'paragraphs are the': 1, 'are the building': 1, 'the building blocks': 1, 'building blocks of': 1, 'blocks of papers': 1, 'many students define': 1, 'students define paragraphs': 1, 'define paragraphs in': 1, 'paragraphs in terms': 1, 'in terms of': 1, 'terms of length': 1, 'of length a': 1, 'length a paragraph': 1, 'a paragraph is': 4, 'paragraph is a': 2, 'is a group': 1, 'a group of': 2, 'group of at': 1, 'of at least': 1, 'at least five': 1, 'least five sentences': 1, 'paragraph is half': 1, 'is half a': 1, 'half a page': 1, 'a page long': 1, 'the unity and': 1, 'unity and coherence': 1, 'and coherence of': 1, 'coherence of ideas': 1, 'of ideas among': 1, 'ideas among sentences': 1, 'among sentences is': 1, 'sentences is what': 1, 'is what constitutes': 1, 'what constitutes a': 1, 'constitutes a paragraph': 1, 'paragraph is defined': 1, 'is defined as': 1, 'defined as a': 1, 'as a group': 1, 'group of sentences': 2, 'of sentences or': 1, 'sentences or a': 1, 'or a single': 1, 'a single sentence': 1, 'single sentence that': 1, 'sentence that forms': 1, 'that forms a': 1, 'forms a unit': 1, 'length and appearance': 1, 'and appearance do': 1, 'appearance do not': 1, 'do not determine': 1, 'not determine whether': 1, 'determine whether a': 1, 'whether a section': 1, 'a section in': 1, 'section in a': 1, 'in a paper': 1, 'a paper is': 1, 'paper is a': 1, 'is a paragraph': 1, 'in some styles': 1, 'some styles of': 1, 'styles of writing': 1, 'particularly journalistic styles': 1, 'a paragraph can': 1, 'paragraph can be': 1, 'can be just': 1, 'be just one': 1, 'just one sentence': 1, 'one sentence long': 1, 'is a sentence': 1, 'a sentence or': 1, 'sentence or group': 1, 'or group of': 1, 'of sentences that': 1, 'sentences that support': 1, 'that support one': 1, 'support one main': 1, 'one main idea': 1, 'in this handout': 1, 'we will refer': 1, 'will refer to': 1, 'refer to this': 1, 'to this as': 1, 'this as the': 1, 'as the controlling': 1, 'the controlling idea': 1, 'because it controls': 1, 'it controls what': 1, 'controls what happens': 1, 'what happens in': 1, 'happens in the': 1, 'in the rest': 1, 'the rest of': 1, 'rest of the': 1, 'of the paragraph': 1} \n",
      "\n",
      "score of ('paragraphs','are') is 0.13221153846153846\n",
      "score of ('paragraphs','of') is  0.04326923076923077\n"
     ]
    }
   ],
   "source": [
    "# a simple bigram model\n",
    "\n",
    "import re\n",
    "\n",
    "SPLIT_PATTERN = r\"([!?.,;])\"\n",
    "WORD_PATTERN = r\"[a-zA-Z]+\"\n",
    "\n",
    "CORPUS = \"Paragraphs are the building blocks of papers. Many students define paragraphs in terms of length: a paragraph is a group of at least five sentences, a paragraph is half a page long, etc. In reality, though, the unity and coherence of ideas among sentences is what constitutes a paragraph. A paragraph is defined as “a group of sentences or a single sentence that forms a unit”. Length and appearance do not determine whether a section in a paper is a paragraph. For instance, in some styles of writing, particularly journalistic styles, a paragraph can be just one sentence long. Ultimately, a paragraph is a sentence or group of sentences that support one main idea. In this handout, we will refer to this as the “controlling idea,” because it controls what happens in the rest of the paragraph.\"\n",
    "\n",
    "lines = [] \n",
    "\n",
    "# tokenize\n",
    "for t in re.split(SPLIT_PATTERN, CORPUS):\n",
    "    l = re.findall(WORD_PATTERN,t.lower())\n",
    "    if len(l)>0:\n",
    "        lines.append(l)\n",
    "\n",
    "# build gram table\n",
    "table1Gram = {}\n",
    "table2Gram = {}\n",
    "table3Gram = {}\n",
    "\n",
    "def increment(dictionary, word):\n",
    "    if word in dictionary:\n",
    "        dictionary[word] = dictionary[word] + 1\n",
    "    else:\n",
    "        dictionary[word] = 1\n",
    "        \n",
    "# get ngram table\n",
    "for line in lines:\n",
    "    m = len(line)\n",
    "    for index, word in enumerate(line):\n",
    "        increment(table1Gram,word)\n",
    "        if index < (m - 1):\n",
    "            increment(table2Gram, word + \" \" + line[index+1])\n",
    "        if index < (m - 2):\n",
    "            increment(table3Gram, word + \" \" + line[index+1] + \" \" + line[index+2])\n",
    "            \n",
    "print('table1Gram \\n', table1Gram , '\\n')\n",
    "print('table2Gram \\n', table2Gram , '\\n')\n",
    "print('table3Gram \\n', table3Gram , '\\n')\n",
    "\n",
    "# bigram probabilitys\n",
    "def p_kn_2(pre_word, current_word, d = 0.75):\n",
    "    \n",
    "    w_i_1_set = [key for key in list(table2Gram.keys()) if key.split(\" \")[0] == pre_word]\n",
    "    w_i_set = [key for key in list(table2Gram.keys()) if key.split(\" \")[1] == current_word]\n",
    "\n",
    "    low_order_weight = d / table1Gram[pre_word] * len(w_i_1_set)\n",
    "    p_continuation = len(w_i_set) / len(table2Gram.keys())\n",
    "    \n",
    "    w_i_1_w_i = pre_word + \" \" + current_word\n",
    "    c = table2Gram[w_i_1_w_i] if w_i_1_w_i in table2Gram else 0\n",
    "    return max(c-d, 0) / table1Gram[pre_word] + low_order_weight * p_continuation\n",
    "\n",
    "print(\"score of ('paragraphs','are') is\", p_kn_2('paragraphs','are'))\n",
    "print(\"score of ('paragraphs','of') is \", p_kn_2('paragraphs','of'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. general recursive formulation\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-n+1}^{i-1}) = \\frac{max(C_{KN}(w_{i-n+1}^{i}) -d, 0) }{C_{KN}(w_{i-n+1}^{i-1})}  + \\lambda(w_{i-n+1}^{i-1}) P_{KN}(w_i | w_{i-n+2}^{i-1}) \n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$ \n",
    "C_{KN}(\\bullet) = \\left \\{\n",
    "\\begin{aligned}\n",
    "    count(\\bullet) \\ for \\ the \\ highest \\ order \\\\\n",
    "    continutation \\ count(\\bullet) \\ for \\ lower \\  order \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "### modified Kneser-Ney smoothing\n",
    "\n",
    "$$\n",
    "P_{KN}(w_i | w_{i-n+1}^{i-1}) = \n",
    "    \\frac\n",
    "        {C(w_{i-n+1}^{i}) - D(C(w_{i-n+1}^{i}))}\n",
    "        {\\sum_{w_i} C(w_{i-n+1}^i)}  + \\lambda(w_{i-n+1}^{i-1}) P_{KN}(w_i | w_{i-n+2}^{i-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "full \\ gram \\ probability = \n",
    "    \\frac\n",
    "        {full \\ gram \\ count - discount}\n",
    "        {prefix \\ sum}\n",
    "    + backoff(prefix)* suffix \\ probability\n",
    "$$\n",
    "\n",
    "\n",
    "where\n",
    "$$\n",
    "D(c) = \\left \\{\n",
    "\\begin{aligned}\n",
    "    & 0 \\ & if \\ c=0 \\\\\n",
    "    & D_1 \\ & if \\ c=1 \\\\\n",
    "    & D_2 \\ & if \\ c=2 \\\\\n",
    "    & D_{3+} \\ & if \\ c\\geq3 \\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_1 = 1 -2 Y \\frac{n_2}{n_1} \\\\\n",
    "D_2 = 2 -3 Y \\frac{n_3}{n_2} \\\\\n",
    "D_{3+} = 3 -4 Y \\frac{n_4}{n_3} \\\\\n",
    "Y = \\frac{n_1}{n_1 + 2n_2}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\lambda(w_{i-n+1}^{i-1}) = \n",
    "\\frac\n",
    "    {D_1N_1(w_{i-n+1}^{i-1}\\cdot)+D_2N_2(w_{i-n+1}^{i-1}\\cdot)+D_{3+}N_{3+}(w_{i-n+1}^{i-1}\\cdot)}\n",
    "    {\\sum_{w_i}c(w_{i-n+1}^i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "backoff(prefix) = \n",
    "\\frac\n",
    "    {discount * cardinality \\ of \\ gram \\ set \\ with \\ prefix}\n",
    "    {prefix \\ sum }\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "N_{1+}(\\bullet w_{i-n+2}^i) = |\\{w_{i-n+1} : C(w_{i-n+1}^i > 0\\}|\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_1(w_{i-n+1}^{i-1} \\cdot) = |{w_i : c(w_{i-n+1}^{i-1}w_i) = 1}|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count k grams where 1<=k<=n\n",
    "# return [1_gram_counter, 2_gram_counter, ...]\n",
    "def gram_count(sentences, n, left_pad=\"<s>\", right_pad=\"</s>\"):\n",
    "    counter = []\n",
    "    for i in range(1,n+1):\n",
    "        ng = Counter()\n",
    "        for sentence in sentences:\n",
    "            sentence_with_pad = [left_pad] * (i-1) + sentence + [right_pad] * (i-1)\n",
    "            for j in range(len(sentence) + i-1):\n",
    "                ng[tuple(sentence_with_pad[j:j+i])] +=1\n",
    "        counter.append(ng)\n",
    "    return counter\n",
    "\n",
    "\n",
    "# total number of grams that have exact <key> count \n",
    "# return Counter({1: 1481, 2: 464, 3: 219, 4: 129})\n",
    "def total_exact_count(gram_count):\n",
    "    tec = Counter()\n",
    "    for counter in gram_count:  \n",
    "        for value in counter.values():\n",
    "            tec[value] += 1\n",
    "    return tec\n",
    "    \n",
    "    \n",
    "# from equation D(c)\n",
    "def discount(total_exact_count,c):\n",
    "    n = total_exact_count\n",
    "    Y = n[1] / (n[1]+2*n[2])\n",
    "    D0 = 0\n",
    "    D1 = 1-2*Y*n[2]/n[1]\n",
    "    D2 = 2-3*Y*n[3]/n[2]\n",
    "    D3 = 3-4*Y*n[4]/n[3]\n",
    "    if c == 0:\n",
    "        return D0\n",
    "    elif c == 1:\n",
    "        return D1\n",
    "    elif c == 2:\n",
    "        return D2\n",
    "    elif c >= 3:\n",
    "        return D3\n",
    "\n",
    "# \n",
    "def prefix_sum(gram_count):\n",
    "    ps = defaultdict(int)\n",
    "    for counter in gram_count:\n",
    "        for gram in counter.keys():\n",
    "            prefix = gram[:-1]\n",
    "            ps[prefix] += counter[gram]\n",
    "    return ps\n",
    "        \n",
    "# \n",
    "def backoff(gram_count, total_exact_count, prefix_sum):\n",
    "    bo = defaultdict(int)\n",
    "    for counter in gram_count:\n",
    "        for key in counter.keys():\n",
    "            value = counter[key]\n",
    "            prefix = key[:-1]\n",
    "            bo[prefix] += discount(total_exact_count, value)\n",
    "    for prefix in bo.keys():\n",
    "        bo[prefix] = bo[prefix] / prefix_sum[prefix]\n",
    "    return bo\n",
    "\n",
    "# build a dict from unigram to n-gram using the recursive formulation\n",
    "# return a dict { gram: kn_prob,}\n",
    "def language_model(gram_count,total_exact_count, prefix_sum, backoff):\n",
    "    lm = defaultdict(int)\n",
    "    for counter in gram_count:\n",
    "        for key in counter.keys():\n",
    "            if len(key) == 1:\n",
    "                # unigram Kneser-Ney probability\n",
    "                total_diff_precede = len([k for k in gram_count[1].keys() if k[1] == key[0]])\n",
    "                total_bigram_type = len(gram_count[1].keys())\n",
    "                lm[key] = total_diff_precede / total_bigram_type\n",
    "            else:\n",
    "                # bigram, trigram, ..\n",
    "                c = counter[key]\n",
    "                D = discount(total_exact_count,c)\n",
    "                prefix = key[:-1]\n",
    "                suffix = key[1:]\n",
    "                \n",
    "                # recursive formulation\n",
    "                lm[key] = (c - D) / prefix_sum[prefix] + backoff[prefix] * lm[suffix]\n",
    "    return lm\n",
    "\n",
    "\n",
    "def p_kn(language_model, backoff, n, sentence):\n",
    "    \n",
    "    # check lm for kn prob\n",
    "    # if not found(first term is zero) and prefix is not in corpus (backoff weight undefined)\n",
    "    # backoff completely to lower n-gram \n",
    "    # see first edge case\n",
    "    # in http://www.cs.cmu.edu/~tbergkir/11711fa16/recitation1.pdf\n",
    "    def p(s):\n",
    "        gamma = 1\n",
    "        for i in range(len(s)):\n",
    "            if s[i:len(s)] in language_model:\n",
    "                return math.log(language_model[s[i:len(s)]])\n",
    "        return 0\n",
    "        raise Exception(\"OOV word found in \"+\" \".join(s))\n",
    "\n",
    "    prob = 0\n",
    "    if len(sentence) < n:\n",
    "        return prob\n",
    "    for i in range(len(sentence)-n+1):\n",
    "        partial = sentence[i:i+n]\n",
    "        prob += p(partial)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ['CHAPTER', 'I'], ['Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'], ['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.'], ['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.'], ['Sixteen', 'years', 'had', 'Miss', 'Taylor', 'been', 'in', 'Mr', '.', 'Woodhouse', \"'\", 's', 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',', 'very', 'fond', 'of', 'both', 'daughters', ',', 'but', 'particularly', 'of', 'Emma', '.'], ['Between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.'], ['Even', 'before', 'Miss', 'Taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', ',', 'the', 'mildness', 'of', 'her', 'temper', 'had', 'hardly', 'allowed', 'her', 'to', 'impose', 'any', 'restraint', ';', 'and', 'the', 'shadow', 'of', 'authority', 'being', 'now', 'long', 'passed', 'away', ',', 'they', 'had', 'been', 'living', 'together', 'as', 'friend', 'and', 'friend', 'very', 'mutually', 'attached', ',', 'and', 'Emma', 'doing', 'just', 'what', 'she', 'liked', ';', 'highly', 'esteeming', 'Miss', 'Taylor', \"'\", 's', 'judgment', ',', 'but', 'directed', 'chiefly', 'by', 'her', 'own', '.'], ['The', 'real', 'evils', ',', 'indeed', ',', 'of', 'Emma', \"'\", 's', 'situation', 'were', 'the', 'power', 'of', 'having', 'rather', 'too', 'much', 'her', 'own', 'way', ',', 'and', 'a', 'disposition', 'to', 'think', 'a', 'little', 'too', 'well', 'of', 'herself', ';', 'these', 'were', 'the', 'disadvantages', 'which', 'threatened', 'alloy', 'to', 'her', 'many', 'enjoyments', '.']]\n",
      "-9.469622969906265\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# use gutenberg as corpus\n",
    "sentences = [s for s in gutenberg.sents()][0:1000]\n",
    "\n",
    "n = 3\n",
    "gc = gram_count(sentences,n)\n",
    "tec = total_exact_count(gc)\n",
    "ps = prefix_sum(gc)\n",
    "bo = backoff(gc,tec,ps)\n",
    "lm = language_model(gc,tec,ps,bo)\n",
    "\n",
    "print(p_kn(lm,bo,n,('Emma','by', 'is', 'Austen')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 61 Speech and Language Processing\n",
    "\n",
    "...\n",
    "The highest accuracy language models at the time of this writing make use of\n",
    "neural nets **neural nets**. \n",
    "The problem with standard language models is that **the number of parameters\n",
    "increases exponentially** as the n-gram order increases, and n-grams have no\n",
    "way to generalize from training to test set. Neural networks instead project words\n",
    "into a continuous space in which words with similar contexts have similar representations.\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
