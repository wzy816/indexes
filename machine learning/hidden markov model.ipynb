{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM\n",
    "    Hidden Markov Model\n",
    "\n",
    "## Reference\n",
    "- https://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf\n",
    "- https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\n",
    "\n",
    "## Notation\n",
    "    - A: state transition matrix\n",
    "    - B: observation probability / emission matrix\n",
    "    - O: observation sequence\n",
    "    - PI: initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0.7, 0.3], \n",
    "    [0.4, 0.6]])\n",
    "B = np.array([\n",
    "    [0.1, 0.4, 0.5], \n",
    "    [0.7, 0.2, 0.1]])\n",
    "\n",
    "O = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "    2])\n",
    "\n",
    "PI = np.array([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "    An Evaluation Problem\n",
    "\n",
    "### Description\n",
    "    given A,B,PI and O, compute p\n",
    "    \n",
    "### Solution\n",
    "    forward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha \n",
      " [[0.06      0.28     ]\n",
      " [0.0616    0.0372   ]\n",
      " [0.0058    0.02856  ]\n",
      " [0.007742  0.0018876]] \n",
      "\n",
      "forward probability 0.009629599999999997\n"
     ]
    }
   ],
   "source": [
    "# 计算前向变量矩阵 alpha\n",
    "def forward(a,b,pi,o):\n",
    "    T = len(o)\n",
    "    N = a.shape[0]\n",
    "    alpha = np.zeros((T,N))\n",
    "    alpha[0] = pi * b[:, o[0]]\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        alpha[t] = alpha[t-1].dot(a) * b[:, o[t]]\n",
    "    return alpha\n",
    "\n",
    "alpha = forward(A,B,PI,O)\n",
    "forward_probability = alpha[-1].sum()\n",
    "print('alpha \\n', alpha, '\\n')\n",
    "print('forward probability', forward_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "    A Decoding / Prediction Problem\n",
    "\n",
    "### Description\n",
    "    given A,B,PI,O, find optimal state sequence\n",
    "    \n",
    "### Solution\n",
    "    viterbi algorithm,  a DP algorithm\n",
    "    should yield same result as that from brute force by compute p of all possible path with forward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi\n",
      " [1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def viterbi(a,b,pi,o):\n",
    "    T = len(o)\n",
    "    N = a.shape[0]\n",
    "    delta = np.zeros((T,N))\n",
    "    psi = np.zeros((T,N))\n",
    "    delta[0] = pi * b[:, o[0]]\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for i in range(N):\n",
    "            delta[t,i] = np.max(delta[t-1]* a[:,i]) * b[i, o[t]] \n",
    "            psi[t,i] = np.argmax(delta[t-1]* a[:,i])\n",
    "    \n",
    "    # backtrack\n",
    "    i_star = np.zeros(T,dtype=np.int32)\n",
    "    i_star[T-1] = np.argmax(delta[T-1])\n",
    "    for t in range(T-2,-1,-1): \n",
    "        i_star[t] = psi[t+1, i_star[t+1]]\n",
    "    \n",
    "    return i_star\n",
    "\n",
    "v = viterbi(A,B,PI,O)\n",
    "print('viterbi\\n', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "    A Learning Problem\n",
    "\n",
    "### Description\n",
    "    given O, total number of distinct states(A.shape), total number of distinct observations(B.shape), find optimal state sequence\n",
    "    \n",
    "### Solution\n",
    "    forward-backward algorithm, aka Baum-Welch algorithm, a particular case of EM(Expectation-Maximization) algorithm\n",
    "\n",
    "```\n",
    "\"Unsupervised training of a HMM involves running forward and backward to estimate the *expected* probability of taking various state sequences, then updates the model to match these *expectations*. This repeats many times until things stabilise (covergence). Note that it's non-convex, so the starting point often affects the converged solution.\" --from https://people.eng.unimelb.edu.au/tcohn/comp90042/HMM.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta\n",
      " [[0.0302  0.02792]\n",
      " [0.0812  0.1244 ]\n",
      " [0.38    0.26   ]\n",
      " [1.      1.     ]] \n",
      "\n",
      "backward_probability\n",
      " 0.009629599999999999\n"
     ]
    }
   ],
   "source": [
    "# 计算后向变量矩阵 beta\n",
    "def backward(a,b,pi,o):\n",
    "    T = len(o)\n",
    "    N = a.shape[0]\n",
    "    beta = np.zeros((T,N))\n",
    "    beta[T-1] = np.ones((N))\n",
    "\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(N):\n",
    "            beta[t,i] = np.sum( a[i, :] * b[:, o[t+1]] * beta[t+1,:])\n",
    "\n",
    "    # marginalized_likelihood = np.sum(pi * b[:,o[0]] * beta[0,:]) \n",
    "    return beta\n",
    "\n",
    "beta = backward(A,B,PI,O)\n",
    "print('beta\\n', beta, '\\n')\n",
    "backward_probability = np.sum(PI * B[:,O[0]] * beta[0,:]) \n",
    "print('backward_probability\\n',backward_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch(init_a, init_b, init_pi, observations, iterations):\n",
    "    a = init_a\n",
    "    b = init_b\n",
    "    pi = init_pi\n",
    "    T = len(observations)\n",
    "    N = init_a.shape[1]\n",
    "    M = init_b.shape[1]\n",
    "    \n",
    "    last_p = 0\n",
    "    last_i = 0\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        # E-step\n",
    "        alpha = forward(a, b, pi, observations)\n",
    "        beta = backward(a, b, pi, observations)\n",
    "        gamma = np.zeros((T,N))\n",
    "        for t in range(T):\n",
    "            gamma[t,:] = np.multiply(alpha[t, :], beta[t,:]) / alpha[-1].sum()\n",
    "        ksi = np.zeros([T-1,N,N])\n",
    "        for t in range(T-1):\n",
    "            for i in range(N):\n",
    "                for j in range(N):                   \n",
    "                    ksi[t,i,j] = alpha[t,i] * a[i,j] * beta[t+1,j] * b[j,observations[t+1]] / alpha[-1].sum()\n",
    "\n",
    "        p = alpha[-1].sum()\n",
    "        if p > last_p:\n",
    "            last_p = p\n",
    "            last_i = iteration\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # M-step\n",
    "        new_a = np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                new_a[i,j] = np.sum(ksi[:,i,j]) / np.sum(gamma[0:-1,i])\n",
    "        # same as\n",
    "        # new_a = np.sum(ksi,axis=0) / np.sum(gamma,axis=0).reshape((-1,1)))\n",
    "\n",
    "        new_b = np.zeros((N,M))\n",
    "        for k in range(M):\n",
    "            for j in range(N):\n",
    "                new_b[j,k] = np.sum(gamma[observations == k, j]) / np.sum(gamma[:,j])\n",
    "        \n",
    "        new_pi = gamma[0]\n",
    "\n",
    "        a = new_a\n",
    "        b = new_b\n",
    "        pi = new_pi\n",
    "                     \n",
    "    return a,b,pi,last_p, last_i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations [0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
      " 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "\n",
    "def generate_training_data(a, b, pi, T):\n",
    "    \n",
    "    gen = lambda dist: np.where(np.random.multinomial(1,dist) == 1)[0][0]\n",
    "    \n",
    "    states = np.zeros(T, dtype=np.int32)\n",
    "    observations = np.zeros(T, dtype=np.int32)\n",
    "    states[0] = gen(pi)\n",
    "    observations[0] = gen(b[states[0],:])\n",
    "    \n",
    "    for t in range(1,T):\n",
    "        states[t] = gen(a[states[t-1],:])\n",
    "        observations[t] = gen(b[states[t],:])\n",
    "        \n",
    "    return observations\n",
    "\n",
    "\n",
    "# generate data\n",
    "true_a = np.array([\n",
    "    [0.5, 0.5], \n",
    "    [0.3, 0.7]])\n",
    "true_b = np.array([\n",
    "    [0.3, 0.7], \n",
    "    [0.8, 0.2]])\n",
    "true_pi = np.array([0.2, 0.8])\n",
    "\n",
    "T = 100\n",
    "\n",
    "observations = generate_training_data(true_a, true_b, true_pi, T)\n",
    "\n",
    "true_p = forward(true_a, true_b, true_pi, observations)[-1].sum()\n",
    "\n",
    "print('observations',observations, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained a\n",
      " [[0.91425981 0.08574019]\n",
      " [0.05378814 0.94621186]]\n",
      "True A\n",
      " [[0.5 0.5]\n",
      " [0.3 0.7]] \n",
      "\n",
      "trained b\n",
      " [[0.94798207 0.05201793]\n",
      " [0.53430714 0.46569286]]\n",
      "True B\n",
      " [[0.3 0.7]\n",
      " [0.8 0.2]] \n",
      "\n",
      "trained pi\n",
      " [1.00000000e+00 9.53128917e-39]\n",
      "True PI\n",
      " [0.2 0.8] \n",
      "\n",
      "trained forward probability\n",
      " 6.535518988634797e-26 \n",
      "\n",
      "trained forward probability2\n",
      " 6.535518988634767e-26 \n",
      "\n",
      "True forward probability\n",
      " 1.5000935489976903e-27 \n",
      "\n",
      "trained iteration 77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "init_a = np.array([\n",
    "    [0.9, 0.1], \n",
    "    [0.1, 0.9]])\n",
    "init_b = np.array([\n",
    "    [0.8, 0.2], \n",
    "    [0.2, 0.8]])\n",
    "init_pi = np.array([0.5, 0.5])\n",
    "\n",
    "train_a, train_b, train_pi, train_p, train_i = baum_welch(init_a, init_b, init_pi, observations, 100)\n",
    "\n",
    "print('trained a\\n',train_a)\n",
    "print('True A\\n', true_a, '\\n')\n",
    "print('trained b\\n',train_b)\n",
    "print('True B\\n', true_b, '\\n')\n",
    "print('trained pi\\n',train_pi)\n",
    "print('True PI\\n', true_pi, '\\n')\n",
    "print('trained forward probability\\n',train_p, '\\n')\n",
    "print('trained forward probability2\\n',train_p2, '\\n')\n",
    "print('True forward probability\\n', true_p, '\\n')\n",
    "print('trained iteration', train_i, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
