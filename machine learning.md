# machine learning

## General & Critical

- Deep Learning: A Critical Appraisal :book:
- [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
- [Model Tuning and the Bias-Variance Tradeoff](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)
- [Using Artificial Intelligence to Augment Human Intelligence](https://distill.pub/2017/aia/)
- [Weight Agnostic Neural Networks ADAM GAIER Google Brain](https://weightagnostic.github.io/) :book:
- a mostly complete chart of neural networks :book:
- [Thought as a Technology](http://cognitivemedium.com/tat/index.html)
- <https://github.com/Avik-Jain/100-Days-Of-ML-Code>
- Pattern Recognition and Machine Learning, Christopher M. Bishop,
- Graphical Models Exponential Families and Variational Inference
- Machine Learning A Probabilistic Perspective
- Probabilistic Graphical Models Principles and Techniques
  - <https://www.youtube.com/playlist?list=PL50E6E80E8525B59C>
- Convex Optimization, Stephen Boyd
- Artificial intelligence: A Modern Approach, Stuart Jonathan Russell 和 Peter Norvig
- [deep learning](https://www.deeplearningbook.org/)
- 《集体智慧编程》
- 《An Introduction to Statistical Learning》
- 《the elements of statistical learning》
- [What’s Really Going On in Machine Learning? Some Minimal Models](https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/)
- The Platonic Representation Hypothesis
  - [paper](https://arxiv.org/pdf/2405.07987)
  - [kimi](kimi/The%20Platonic%20Representation%20Hypothesis.md)

## Bayesian

- [Conditional probability](https://setosa.io/ev/conditional-probability/)
- ZhuSuan: A Library for Bayesian Deep Learning :book:
- Gaussian Processes for Machine Learning :book:
- [Categorical Reparameterization with Gumbel-Softmax](https://www.youtube.com/watch?v=JFgXEbgcT7g) :book:
- [The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables](https://arxiv.org/abs/1611.00712)
  - <https://github.com/ericjang/gumbel-softmax>
- [Bayesian Methods for Media Mix Modeling with Carryover andShape Effects](https://storage.googleapis.com/gweb-research2023-media/pubtools/3806.pdf)
- [Challenges And Opportunities In Media Mix Modeling](https://storage.googleapis.com/gweb-research2023-media/pubtools/3803.pdf)
- [Geo-level Bayesian Hierarchical Media Mix Modeling](https://storage.googleapis.com/gweb-research2023-media/pubtools/3804.pdf)
- 《Statistical Rethinking》 by Richard McElreath
  - <https://xcelab.net/rm/>
  - [numpyro](https://fehiepsi.github.io/rethinking-numpyro/)
  - [course and book](https://github.com/rmcelreath/rethinking)

## Hidden Markov Models

- A Revealing Introduction to Hidden Markov Models :book:

### Monte Carlo

- [A Survey of Monte Carlo Tree Search Methods](http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf)

## Conditional Random Fields

- Conditional Random Fields Probabilistic Models for Segmenting and Labeling Sequence Data :book:

## RBM

- [A Beginner's Guide to Restricted Boltzmann Machines (RBMs)](https://wiki.pathmind.com/restricted-boltzmann-machine)

## CNN

- [Example of 2D Convolution](http://www.songho.ca/dsp/convolution/convolution2d_example.html)
- [MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEURAL NETWORKS](https://arxiv.org/pdf/1412.6564)

## RNN

- DRAW: A Recurrent Neural Network For Image Generation :book:
  - use spatial attention + sequential vae to generate image
  - <https://github.com/ericjang/draw>
  - <https://github.com/ikostrikov/TensorFlow-VAE-GAN-DRAW>
  - <https://github.com/conan7882/DRAW-recurrent-image-generation>
- A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction :book:
  - DA-RNN
  - <https://github.com/Zhenye-Na/DA-RNN>
- Opinion Mining with Deep Recurrent Neural Networks :book:
- [Generating Abstract Patterns with TensorFlow](https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/)
- [Recurrent Neural Network Tutorial for Artists](https://blog.otoro.net/2017/01/01/recurrent-neural-network-artist/)
- [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45) :book:
- Bidirectional Recurrent Neural Networks :book:
- Multi-Dimensional Recurrent Neural Networks :book:
- Training and Analyzing Deep Recurrent Neural Networks :book:
- LEARNING STOCHASTIC RECURRENT NETWORKS :book:
  - propose SRNN
- A Recurrent Latent Variable Model for Sequential Data
  - propose VRNN, an extension of VAE to support sequence data
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- LONG SHORT-TERM MEMORY :book:

## Time Series

- [Awesome Time Series Papers](https://github.com/bighuang624/Time-Series-Papers)

## t-SNE & PCA

- Visualizing Data using t-SNE :book:
- Mixtures of Probabilistic Principal Component Analysers :book:

## Clustering

- A Survey of Correlation Clustering :book:
- Correlation Clustering :book:

## Kernel Density Estimation

- [Understanding Kalman Filters, Part 1: Why Use Kalman Filters?](https://mathisonian.github.io/kde/) :movie_camera: :framed_picture:

## Kalmen Filters

- <https://www.youtube.com/watch?v=mwn8xhgNpFY>

## SVM

- One-Class SVMs for Document Classification :book:

## Mixture Density Networks

- [Mixture Density Networks with TensorFlow](https://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/)
- <http://edwardlib.org/tutorials/mixture-density-network>
- [Mixture Density Networks for Galaxy distance determination in TensorFlow](http://cbonnett.github.io/MDN.html)
- [What My Deep Model Doesn't Know...](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)

## Gaussian Mixture Model

- [Gaussian Mixture Models Explained](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95)
- [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)
- [Mixture Models 3: multivariate Gaussians](https://www.youtube.com/watch?v=TG6Bh-NFhA0)
- [Mixture Models 5: how many Gaussians?](https://www.youtube.com/watch?v=BWXd5dOkuTo)

## EM

- [Expectation Maximization: how it works](https://www.youtube.com/watch?v=iQoXFmbXRJA)

## Pointer Network

- tf
  - <https://github.com/devsisters/pointer-network-tensorflow>
  - <https://github.com/keon/pointer-networks>
  - <https://github.com/ikostrikov/TensorFlow-Pointer-Networks/blob/master/main.ipynb>
  - <https://github.com/devnag/tensorflow-pointer-networks>
- keras
  - <https://github.com/preddy5/seq2set-keras>
- sentence ordering
  - <https://github.com/JerrikEph/SentenceOrdering_PTR>

## VAE

- <https://github.com/debasishg/ml-readings/blob/master/vae.md>
- <https://github.com/matthewvowels0/Awesome-VAEs>
- Recent Advances in Autoencoder-Based Representation Learning :book:
  - good overview
  - discussed tradeoff btw reconstruction and KL divergence
  - need prior and defined downstream task for unsupervised learning
- <https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/>
- <https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/> on CVAE
- <https://www.countbayesie.com/blog/2016/5/9/kullback-leibler-divergence-explained> on KL divergence
- [Reparametrization Trick](https://gabrielhuang.gitbooks.io/machine-learning/content/reparametrization-trick.html)
- Notes on Variational Autoencoders :book:
- [From Autoencoder to Beta-VAE](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)
- Ladder Variational Autoencoders :book:
  - propose LVAE
  - <https://github.com/ermongroup/Variational-Ladder-Autoencoder>
  - <https://github.com/Michedev/VLAE>
  - <https://github.com/davidsandberg/LadderVAE>
- PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES :book:
  - <https://github.com/igul222/PixelVAE>
  - <https://github.com/kundan2510/pixelVAE>
- VARIATIONAL LOSSY AUTOENCODER :book:
  - propose VAE+autoregressive model
  - use autogressive flow as prior
  - <https://github.com/jiamings/tsong.me/blob/master/_posts/reading/2016-11-08-lossy-vae.md>
- DEEP VARIATIONAL INFORMATION BOTTLENECK :book:
  - modified objective
  - <https://github.com/AliLotfi92/Deep_Variational_Information_Bottleneck>
  - <https://github.com/alexalemi/vib_demo>
- VAE with a VampPrior :book:
  - VampPrior = approximated aggregated posterior as the optimal prior
  - related to CEVAE
  - VAE with a VampPrior presentation :book:
  - <https://github.com/belaalb/CEVAE-VampPrior>
  - <https://github.com/jmtomczak/vae_vampprior>
- InfoVAE: Balancing Learning and Inference in Variational Autoencoders :book:
  - MMD-VAE
  - objective function is general for beta-VAE and AAE
  - <https://github.com/ShengjiaZhao/InfoVAE>
- Learning Structured Output Representation using Deep Conditional Generative Models :book:
  - Conditional VAE + Gaussian Stocastic NN
  - <https://github.com/wsjeon/ConditionalVariationalAutoencoder>
- Variational Inference with Normalizing Flows :book:
  - new ways of constructing posterior
- [Normalizing Flows](http://akosiorek.github.io/ml/2018/04/03/norm_flows.html)
- Neural Discrete Representation Learning
  - propose VQ-VAE
  - encoder output is discrete
  - to avoid posterior collapse
  - reconstruction demo at <https://avdnoord.github.io/homepage/vqvae/>
  - <https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py>
  - <https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb>
- Generating Diverse High-Fidelity Images with VQ-VAE-2 :book:
  - based on hierachical VQ-VAE
  - 2-stage approach: 1. Train a hierarchical VQ-VAE 2. fit a PixelCNN in prior
  - <https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb>
  - <https://github.com/rosinality/vq-vae-2-pytorch>
  - <https://github.com/unixpickle/vq-vae-2>
- SOM-VAE INTERPRETABLE DISCRETE REPRESENTATION LEARNING ON TIME SERIES :book:
  - user self-organizing map and markov in latent space
  - <https://github.com/KurochkinAlexey/SOM-VAE>
  - <https://github.com/ratschlab/SOM-VAE>
- Filtering Variational Objectives :book:
- Hierarchical Variational Autoencoders for Music :book:
- A Neural Representation of Sketch Drawings :book:
  - propose Seq2seq vae to process sketch sequence
  - Encoder is Bidirectional LSTM RNN
  - Decoder is autoregressive HyperLSTM Ron
  - <https://magenta.tensorflow.org/sketch_rnn>
- Tutorial on Variational Autoencoders :book:
- Auto-Encoding Variational Bayes :book:
- [Lecture 13 Generative Modelse](https://www.youtube.com/watch?v=5WoItGTWV54&t=1592s) :book
- [Kullback-Leibler Divergence Explaine](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
- [Autoencoders and Representation Learning](https://www.youtube.com/watch?v=R3D/NKE3zKFk&t=116s)
- LATENT CONSTRAINTS: LEARNING TO GENERATE CONDITIONALLY FROM UNCONDITIONAL GENERATIVE MODELS :book:
  - introducing contraint in training
- VARIATIONAL AUTOENCODER WITH ARBITRARY CONDITIONING :book:
  - propose conditional VAE base on features
  - two applications: feature imputation and image inpainting
- CS598LAZ - Variational Autoencoders :book:
- VAE_literature :book:
- Demystifying Variational Autoencoders :book:
- β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK :book:
  - <https://github.com/google-research/disentanglement_lib>
  - <https://github.com/cianeastwood/qedr>
- Composing graphical models with neural networks for structured representations and fast inference :book:
  - propose SVAE
  - combine pgm and deep learning
  - used hierarchical priors to learn more interpretable latent variables
  - <https://github.com/mattjj/svae>
  - <https://www.youtube.com/watch?v=vnO3w8OgTE8> :movie_camera:
- Disentangled VAE Representations for Multi-Aspect and Missing Data :book:
  - propose factVAE
  - <https://github.com/samuela/faceback>
- Learning to Decompose and Disentangle Representations for Video Prediction :book:
  - propose DDPAE to predict frame
  - <https://github.com/jthsieh/DDPAE-video-prediction>
- Disentangled Sequential Autoencoder :book:
  - one can exploit known relations between factors in different samples
  - latent space has both static and dynamic parts
  - two methods for disentanglement: 1 new objective function (beta-VAE) 2 new network architecture (infoGAN)
- Learning Disentangled Joint Continuous and Discrete Representations
  - propose JointVAE, modify latent variable from beta-vae
  - <https://github.com/Schlumberger/joint-vae>
  - <https://github.com/voxmenthe/JointVAE_v1>
  - propose
- Disentangling by Factorising :book:
  - propose FactorVAE
  - add total correlation on objective function
  - use new way of measuring disentanglement
  - <https://github.com/paruby/FactorVAE>
  - <https://github.com/nicolasigor/FactorVAE>
  - <https://github.com/AliLotfi92/Disentangling_by_Factorising>
- Isolating Sources of Disentanglement in VAEs :book:
  - propose β-TCVAE like FactorVAE
  - use MIG as metric
  - <https://github.com/rtqichen/beta-tcvae>
  - <https://github.com/google-research/disentanglement_lib>
- Causal Effect Inference with Deep Latent-Variable Models
  - propose CEVAE
  - <https://github.com/AMLab-Amsterdam/CEVAE>
- Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations :book:
  - propose multi-level VAE
  - using group-level supervision
  - 2 latent factors for content and type, diff content btw group, diff style inside group
  - <https://github.com/ananyahjha93/multi-level-vae>
- Wasserstein Auto-Encoders :book:
  - propose WAE, a generalized adversarial AE
  - discussed GAN, VAE and AAE
  - review <https://openreview.net/forum?id=HkL7n1-0b>
  - <https://github.com/tolstikhin/wae>
- VARIATIONAL INFERENCE OF DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS :book:
  - propose DIP-VAE
  - modify ELBO by adding a regularization term to minimises the covariance between the latents
  - avoid tradeoff in beta-VAE between reconstruction and disentanglement
  - chapter 2 formulation is fantastic
  - review at <https://openreview.net/forum?id=H1kG7GZAW>
  - <https://github.com/paruby/DIP-VAE>
  - <https://github.com/IBM/AIX360/blob/master/aix360/algorithms/dipvae/dipvae.py>
  - <https://github.com/google-research/disentanglement_lib/blob/master/disentanglement_lib/methods/unsupervised/vae.py>
  - <https://github.com/ethanluoyc/pytorch-vae>

## Representation Learning & Disentanglement

- <https://github.com/google-research/disentanglement_lib>
- <https://github.com/sootlasten/disentangled-representation-papers>
- CENSORING REPRESENTATIONS WITH AN ADVERSARY :book:
  - propose adversarially learned fair representation
- THE VARIATIONAL FAIR AUTOENCODER :book:
  - propose VFAE based on VAE and MMD regularize term
  - paper note on <https://ameroyer.github.io/reading-notes/representation%20learning/2019/05/02/the_variational_fair_autoencoder.html>
  - <https://github.com/dendisuhubdy/vfae>
  - <https://github.com/yevgeni-integrate-ai/VFAE>
- Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data
  - propose factorized hierarchical VAE for sequential data
  - main difference is that two encoders for sequence attributes and segment attributes that lead to two latent z
  - trained using a combinatio of the standard variational lower bound and a discriminative regulariser to further encourage disentanglement
  - <https://github.com/wnhsu/FactorizedHierarchicalVAE>
  - Fully-Connected Factorized Hierarchical VAE <https://github.com/wnhsu/FactorizedHierarchicalVAE/blob/github_FHVAE/src/models/fc_fhvae.py>
  - Recurrent Factorized Hierarchical VAE <https://github.com/wnhsu/FactorizedHierarchicalVAE/blob/github_FHVAE/src/models/rec_fhvae.py#L283>
  - Scalable Factorized Hierarchical Variational Autoencoder Training <https://github.com/wnhsu/ScalableFHVAE>
- Learning Deep Disentangled Embeddings With the F-Statistic Loss
  - explained the relationship between deep embedding and disentanglement
  - notes on <https://medium.com/datadriveninvestor/paper-review-learning-deep-disentangled-embeddings-with-the-f-statistic-loss-neurips-2018-b6a84828a24b>
  - <https://github.com/kridgeway/f-statistic-loss-nips-2018>
- A FRAMEWORK FOR THE QUANTITATIVE EVALUATION OF DISENTANGLED REPRESENTATIONS
  - propose a method of disentanglement quantization, based on ground-truth latent structure
  - use regressor to compute three metrics: disentanglement, completeness, informativeness
  - review at <https://openreview.net/forum?id=By-7dz-AZ>
  - <https://github.com/cianeastwood/qedr>
- A Heuristic for Unsupervised Model Selection for Variational Disentangled Representation Learning :book:
  - current limit of unsupervised model is itis high optimization variance and heavy reliance on hyperparameter and initialization seed
  - propose a method of model selection: UDR, unsupervised disentanglement ranking
  - review <https://openreview.net/forum?id=SyxL2TNtvr>
- Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations :book:
  - this is a large-scale empirical study
  - training disentanglement purely on data is theoretically impossible unless introducing inductive bias
  - <https://github.com/google-research/disentanglement_lib>
- Hierarchical Disentangled Representations :book:
  - detail explaination on ELBO
- From Deep Learning of Disentangled Representations to Higher-level Cognition :movie_camera: :book:
  - <https://www.youtube.com/watch?v=Yr1mOzC93xs>

## GAN

- [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)
- [the gan zoo](https://github.com/hindupuravinash/the-gan-zoo)
- <https://www.tensorflow.org/tutorials/generative/dcgan#%E4%BB%80%E4%B9%88%E6%98%AF%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%EF%BC%9Fo>
- REAL-VALUED MEDICAL TIME SERIES GENERATION WITH RECURRENT CONDITIONAL GANS :book:
  - propose RGAN and RCGAN
  - <https://github.com/ratschlab/RGAN>
- C-RNN-GAN: Continuous recurrent neural networks with adversarial training :book:
  - use LSTM RNN
  - <https://github.com/olofmogren/c-rnn-gan>
- Wasserstein Generative Adversarial Networks :book:
  - use EM earth-mover distance to measure distribution btw two distribution
- Multivariate Time Series Imputation with Generative Adversarial Networks :book:
- [Generative Models](https://openai.com/blog/generative-models/)
- EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES :book:
- Adversarial Autoencoders :book:
  - propose AAE and its relation with AE and GAN
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets :book:
  - canonical paper
  - propose InfoGAN which adds lower bound of mutual information to objective function
  - sim. to wake-sleep algorithm
  - review at <http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips29/reviews/1134.html>
  - note at <https://ameroyer.github.io/reading-notes/representation%20learning/2019/05/02/infovae.html>
  - <https://github.com/openai/InfoGAN>
  - <https://github.com/tensorpack/tensorpack/blob/master/examples/GAN/InfoGAN-mnist.py>
- ADVERSARIALLY LEARNED INFERENCE :book:
  - propose ALI = VAE+GAN, related to AAE and BiGAN
  - notes on <https://ishmaelbelghazi.github.io/ALI/>
  - <https://github.com/IshmaelBelghazi/ALI>
- ADVERSARIAL FEATURE LEARNING :book:
  - propose Bidirectional GAN
  - add encoder in normal GAN to learn inverse mapping
  - use alternating gradient descent to train
  - simple encoder doesn't work for complex dataset, show best result on MNIST
- Disentangling factors of variation in dtheep representations using adversarial training
  - <https://github.com/MichaelMathieu/factors-variation>
  - <https://github.com/ananyahjha93/disentangling-factors-of-variation-using-adversarial-training>

## nlp

- An Introduction to Information Retrieval :book:
  - must read chapter 2 and 6
- [Deep Learning for NLP resources](https://github.com/andrewt3000/dl4nlp)
- syntactic_structures :book:
- Natural Language Processing With Python :book:

### Lib

- [HanLP](https://github.com/hankcs/HanLP)
- [jieba](https://github.com/fxsjy/jieba)
- [swcs](http://www.xunsearch.com/scws/docs.php)
- [paoding-analysis](https://gitee.com/zhzhenqin/paoding-analysis)
- [pangusegment](https://archive.codeplex.com/?p=pangusegment)
- [SnowNLP](https://github.com/isnowfy/snownlp)
- [THULAC 清华](http://thulac.thunlp.org/)
- [FudanNLP 复旦](https://github.com/FudanNLP/fnlp)
- [fastNLP](https://github.com/fastnlp/fastNLP)
- [LTP 哈工大语言云](http://www.ltp-cloud.com/document2#api2_python_interface)
- [NLPIR 北理工](https://github.com/NLPIR-team/NLPIR)
- [StanfordCoreNLP](https://github.com/stanfordnlp/CoreNLP)
- [pkuseg 北大](https://github.com/lancopku/pkuseg-python)
- [baidu LAC](https://github.com/baidu/lac)
- [mmseg](https://pypi.org/project/mmseg/)
- [Boson NLP](http://docs.bosonnlp.com/tag.html)
- [Baidu NLP](https://cloud.baidu.com/doc/NLP/NLP-Python-SDK.html#.3C.CF.E9.5F.9C.E3.C3.45.DA.9C.9E.4C.F8.55.F1.E6)
- [Aliyun](https://help.aliyun.com/document_detail/61384.html?spm=a2c4g.11186623.6.549.27433020OdUf5E)
- [Sougou](http://www.sogou.com/labs/webservice/)
- [Tencent NLP](https://nlp.qq.com/help.cgi?topic=api#analysis)

### Word Embedding Models

- word2vec
  - [fasttext](https://github.com/facebookresearch/fastText#full-documentation)
  - gensim
  - word2vec Parameter Learning Explained
    - explained CBOW, CSG, hierachical softmax, subsampling
  - [A Beginner's Guide to Word2Vec and Neural Word Embeddings](https://wiki.pathmind.com/word2vec)
  - [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
  - [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)
  - [tensorflow core word2vec](https://www.tensorflow.org/tutorials/text/word2vec)
- GloVe
  - GloVe: Global Vectors for Word Representation :book:
  - [website](https://nlp.stanford.edu/projects/glove/)
- Efficient Estimation of Word Representations in Vector Space :book:
- A Neural Probabilistic Language Model :book:
- Distributed Representations of Words and Phrases and their Compositionality
  - proposed hierachical softmax and negative sampling
- An empirical study of smoothing techniques for language modeling :book:
- Deep contextualized word representation :book:
  - propose ELMO
- BERT: Pre-training of Deep Bidirectional Transformers for Languange Understanding :book:

### Word Segmentation

- 现代汉语语料库加工规范-词语切分及词性标注 :book:
- 汉语自动分词研究评述 :book:
- Which is More Suitable for Chinese Word Segmentation, the Generative Model or the Discriminative One? :book:
- 互联网时代的社会语言学：基于 SNS 的文本数据挖掘 :book:

#### 基于匹配/机械分析

1. forward maximal matching MM 正向最大匹配法，最长词优先匹配法
2. reverse maximum matching RMM 逆向最大匹配法
3. bidirectional maximal matching 双向最大匹配
4. 最少分词法
5. 全切分法
6. [MMSEG](http://technology.chtsai.org/mmseg/)

1 和 2 速度快，歧义检测和消解步骤合一，但是需要完备的词典；3 结合 MM 和 RMM，实用性好，但是存在着切分歧义检测盲区；6 优化了最大匹配，基于一个词典，两个匹配规算法，四个消除歧义规则。

#### 基于统计

1. N-gram
2. Hidden Markov Model(HMM)
3. Maximum Entropy, ME
4. Conditional Random Fields，CRF

2 和 4 基于序列标注。

#### 基于语义（理解）的

1. Augmented Transition Network

#### 基于神经网络的

1. LSTM+CRF
2. BiLSTM+CRF

## Text Classification

- Convolutional Neural Networks for Sentence Classification :book:
- Character-level Convolutional Networks for Text Classification :book:
- Bag of Tricks for Efficient Text Classification :book:

## Training & Inference

- Population Based Training of Neural Networks :book:
- [MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/pdf/2402.15627)
  - [kimi](paper/MegaScale:%20Scaling%20Large%20Language%20Model%20Training%20to%20More%20Than%2010,000%20GPUs.md)
- [PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel](https://arxiv.org/pdf/2304.11277)
  - [kimi](paper/PyTorch%20FSDP:%20Experiences%20on%20Scaling%20Fully%20Sharded%20Data%20Parallel.md)
- [Fully Sharded Data Parallel: faster AI training with fewer GPUs](https://engineering.fb.com/2021/07/15/open-source/fsdp/)
- vllm
  - [website](https://docs.vllm.ai/en/v0.4.2/index.html)
  - [paged attention](https://blog.vllm.ai/2023/06/20/vllm.html)
- [Transformer Model Parallelism](https://huggingface.co/docs/transformers/v4.19.4/en/parallelism)
- [Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/perf_train_gpu_many)
- [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU)

## Datasets & Format

- [gguf](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md)
- [UCSD Anomaly Detection Dataset](http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm)
- Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark :book:
  - introduce numenta benchmark dataset and score system
  - <https://github.com/numenta/NAB/tree/master/data>
- Toward Credible Evaluation of Anomaly-Based Intrusion-Detection Methods :book:
  - compare 3 intrusion detection methods: signature-based, anomaly-based and specification-based
- [math23k](https://github.com/SCNU203/Math23k/tree/main)
- [gsm8k](https://huggingface.co/datasets/openai/gsm8k)
- [OpenThoughts3-1.2M](https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M)

## Visualization

- [Understanding Neural Networks Through Deep Visualization](https://yosinski.com/deepvis)

## User Profiling

- Finding Users Who Act Alike: Transfer Learning for Expanding Advertiser Audiences :book: :framed_picture:
  - on user expansion in advertisement
  - learn user embdeding model from log history data
- Adversarial Substructured Representation Learning for Mobile User Profiling :book:
  - use user's check-in data to build graph
  - user adversarial learning for graph representation
- Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion :book:

## Autonomous Driving

- [Learning to Drive in a Day](https://arxiv.org/pdf/1807.00412.pdf)
- [Deep Reinforcement Learning framework for Autonomous Driving](https://arxiv.org/pdf/1704.02532.pdf)
- [MarIQ](https://www.youtube.com/watch?v=Tnu4O_xEmVk)
- [A Survey of Deep Learning Techniques for Autonomous Driving](https://arxiv.org/pdf/1910.07738v2.pdf)
- [NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles](https://arxiv.org/pdf/2106.11810.pdf)
- Planning-oriented Autonomous Driving
  - [paper](https://arxiv.org/pdf/2212.10156)
  - [kimi](paper/Planning-oriented%20Autonomous%20Driving.md)

## transformer

### AIGC

- [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)
- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- A Survey of Large Language Model
  - [paper en](https://arxiv.org/pdf/2303.18223.pdf)
  - [paper cn](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese_0418.pdf)
  - [github](https://github.com/RUCAIBox/LLMSurvey)
- [Planning for AGI and beyond by Sam Altman](https://openai.com/blog/planning-for-agi-and-beyond)
- [ChatGPT 原理介绍：从语言模型走近 chatgpt](https://zhuanlan.zhihu.com/p/608047052)
- [通向 AGI 之路：大型语言模型（LLM）技术精要](https://zhuanlan.zhihu.com/p/597586623)
  - LLM 作为交互接口理解人
  - 模型大小与训练数据量关系 => 涌现
  - 分步拆解，增强了推理 CoT 能力
  - 未来 LLM 模型稀疏化
- [State of GPT](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [A Path Towards Autonomous Machine Intelligence](https://openreview.net/pdf?id=BZ5a1r-kVsf)
  - Mode-1 (perception-action) and Mode-2 (model-predictive control, MPC)
  - propose a Joint Embedding Predictive Architectures for Self-Supervised Learning
- [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)
  - general methods that leverage computation are ultimately the most effective
- [AI engineer](https://www.latent.space/p/ai-engineer)
  - LLM enabled Product
  - software 3.0
- [Levels of AGI: Operationalizing Progress on the Path to AGI](https://arxiv.org/pdf/2311.02462.pdf)
  - define 6 criteria of AGI and 6-level ontology of AGI
- [THE IMPACT OF DEPTH AND WIDTH ON TRANSFORMER LANGUAGE MODEL GENERALIZATION](https://arxiv.org/pdf/2310.19956.pdf)
- [llm app stack](https://github.com/a16z-infra/llm-app-stack)
  - [post](https://a16z.com/emerging-architectures-for-llm-applications/)
- [DREAM-LOGIC, THE INTERNET AND ARTIFICIAL THOUGHT](https://www.edge.org/conversation/david_gelernter-dream-logic-the-internet-and-artificial-thought)
- [llm visualization](https://bbycroft.net/llm)
- [How LLMs Work, Explained Without Math](https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math)
- [《大模型基础》](https://github.com/ZJU-LLMs/Foundations-of-LLMs/) :star:
- LLMs Get Lost In Multi-Turn Conversation
  - [paper](https://arxiv.org/pdf/2505.06120)
  - [kimi](paper/LLMs%20Get%20Lost%20In%20Multi-Turn%20Conversation.md)
- [大规模语言模型：从理论到实践](https://intro-llm.github.io/)
  - [v2](https://intro-llm.github.io/chapter/LLM-TAP-v2.pdf)
- [model atlas](https://huggingface.co/spaces/Eliahu/Model-Atlas)

### Transformer

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [A Simple Example of Causal Attention Masking in Transformer Decoder](https://medium.com/@jinoo/a-simple-example-of-attention-masking-in-transformer-decoder-a6c66757bc7d)
- perceiver io
  - perform self-attention on latent variables, cross-attention on inputs, solve qudratic scaling of seq length
  - [paper](https://arxiv.org/pdf/2107.14795.pdf)
  - [hf doc](https://huggingface.co/docs/transformers/model_doc/perceiver)
  - [deepmind jax implementation](https://github.com/deepmind/deepmind-research/blob/master/perceiver/README.md)
  - [pytorch implementation](https://github.com/krasserm/perceiver-io)
- Attention as an RNN
  - [paper](https://arxiv.org/pdf/2405.13956)
  - [kimi](paper/Attention%20as%20an%20RNN.md)
- Long-Range attention
  - <https://huggingface.co/blog/long-range-transformers>
    - 4 improvements on vanilla attention
  - [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)
    - chapter 2 very canonical
  - [LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS](https://arxiv.org/pdf/2011.04006.pdf)
    - benchmark for evaluating model quality under long-context scenario
    - [github](https://github.com/google-research/long-range-arena)
  - Linformer
    - low-rank, add projection to v & k
    - [paper](https://arxiv.org/pdf/2006.04768.pdf)
    - [Johnson–Lindenstrauss lemm proof](https://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf)
    - [blog](https://tevenlescao.github.io/blog/fastpages/jupyter/2020/06/18/JL-Lemma-+-Linformer.html)
    - [hf blog](https://huggingface.co/blog/long-range-transformers#linformer-self-attention-with-linear-complexity)
  - BigBird
    - block sparse attention up to 4096
    - [hf blog](https://huggingface.co/blog/big-bird#bigbird-block-sparse-attention)
    - [hf code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/big_bird/modeling_big_bird.py#LL514C10-L514C10)
- Rotary Position Encoding
  - [blog CN , key equation 11 and 13](https://kexue.fm/archives/8265)
  - [paper, aligned with blog CN](https://arxiv.org/pdf/2104.09864v4.pdf)
  - [roformer github](https://github.com/ZhuiyiTechnology/roformer)
  - [blog EN from eleuther AI](https://blog.eleuther.ai/rotary-embeddings/)
- Alibi
  - [paper](https://arxiv.org/pdf/2108.12409v2.pdf)
  - [attention implementation](https://github.com/jaketae/alibi/blob/main/alibi/attention.py)

### Mixture of Expert, MoE

- [Mixture of Experts Explained](https://huggingface.co/blog/moe)
- mixtral 8x7B
  -[Mixtral of experts](https://mistral.ai/news/mixtral-of-experts)
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
  - [paper](https://arxiv.org/pdf/2101.03961)
  - [kimi](paper/Switch%20Transformers:%20Scaling%20to%20Trillion%20Parameter%20Models%20with%20Simple%20and%20Efficient%20Sparsity.md)

### Base Model

GPT,LlaMa

- [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
- nanoGPT
  - [github](https://github.com/karpathy/nanoGPT)
- GPT1
  - paper [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- GPT2
  - paper [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  - report [Release Strategies and the Social Impacts of Language Models](https://arxiv.org/pdf/1908.09203.pdf)
  - [release blog](https://openai.com/research/gpt-2-1-5b-release)
  - implementation code
    - [tf by openai](https://github.com/openai/gpt-2/blob/master/src/model.py)
    - [pytorch by huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py)
    - [by nanoGPT](https://github.com/karpathy/nanoGPT/blob/master/model.py)
    - [by cerebras](https://github.com/Cerebras/modelzoo/blob/main/modelzoo/transformers/pytorch/gpt2/gpt2_model.py)
  - GPT2 M
    - [hf](https://huggingface.co/gpt2-medium)
  - GPT2 L
    - [hf](https://huggingface.co/gpt2-large)
  - GPT2 XL
    - [hf](https://huggingface.co/gpt2-xl)
- GPT-J-6B
  - using [Mesh Transformer JAX](https://github.com/kingoflolz/mesh-transformer-jax/) trained on [the pile](https://pile.eleuther.ai/)
  - [hf](https://huggingface.co/EleutherAI/gpt-j-6b)
- GPT3
  - like GPT2 but use alternating dense and locally banded sparse attention patterns
  - paper [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
  - [github](https://github.com/openai/gpt-3)
- CodeX / code-davinci-002
  - GPT3 family on code
  - evaluation paper [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)
- GPT4
  - [blog by openai](https://openai.com/research/gpt-4)
- LlaMa
  - train on roughly 1.4T tokens from public data only
  - norm input at each layer; use SwiGLU; use RoPe
  - performance
    - LLaMa-13B matches GPT3-15B
    - LLaMa-65B matches Chinchilla-70B and PaLM-540B
  - [paper](https://arxiv.org/pdf/2302.13971.pdf)
  - [facebook github](https://github.com/facebookresearch/llama)
  - [download model slowly](https://github.com/shawwn/llama-dl)
  - implementation
    - [hf](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)
    - [facebook](https://github.com/facebookresearch/llama/blob/main/llama/model.py)
  - feedforward
    - [why](https://github.com/facebookresearch/llama/issues/245)
    - [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202.pdf)
  - inference
    - [llama.cpp](https://github.com/ggerganov/llama.cpp)
    - [llama2.c](https://github.com/karpathy/llama2.c)
- DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
  - [paper](https://arxiv.org/pdf/2401.02954)
  - [kimi](paper/DeepSeek%20LLM:%20Scaling%20Open-Source%20Language%20Models%20with%20Longtermism.md)
  1. base model and chat model, 7B 和 67B param, 架构 follow llama， RoPE, GQA
  2. multi-step lr scheduler
  3. scaling law study, model scale = non-embedding flops/token
  4. comprehensive evaluations
- DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
  - [paper](https://arxiv.org/pdf/2405.04434)
  - [kimi](paper/DeepSeek-V2:%20A%20Strong,%20Economical,%20and%20Efficient%20Mixture-of-Experts%20Language%20Model.md)
  - [hf](https://huggingface.co/deepseek-ai/DeepSeek-V2)
  1. Multi-head Latent Attention, low-rank key-value joint compression, decoupled RoPE strategy
  2. DeepSeekMoE
  3. DeepSeek-V2 Chat (SFT), DeepSeek-V2 Chat (RL), DeepSeek-V2-Lite, DeepSeekMath
- DeepSeek-V3 Technical Report
  - [paper](https://arxiv.org/pdf/2412.19437)
  - [kimi](paper/DeepSeek-V3%20Technical%20Report.md)
  - [github](https://github.com/deepseek-ai/DeepSeek-V3)
  - [hf](https://huggingface.co/deepseek-ai/DeepSeek-V3)
  - architecture sim. to V2, MLA+DeepSeekMoE
  - auxiliary-loss-free strategy for load balancing
  - multi-token predicition for training, discard at inference
  - DualPipe
  - FP8 mixed precision training framework,
  - hardware designs
- gpt-oss
  - [compare to gpt2](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)
  - [hf](https://huggingface.co/openai/gpt-oss-20b)

### Supervised Fine-Tuning (SFT) Model

Vicuna,alpaca

- Cerebras-GPT
  - gpt3 style arch with full attention, on the pile data, model size from 111M to 13B
  - [paper](https://arxiv.org/pdf/2304.03208.pdf)
  - [hf 13B](https://huggingface.co/cerebras/Cerebras-GPT-13B)
  - [code](https://github.com/Cerebras/modelzoo/blob/main/modelzoo/transformers/pytorch/gpt3/README.md) missing GPT3 implementation of banded sparse attention
- Vicuna-13B chatbot by LMSYS ORG
  - fine tune LLaMa with 70K user-shared conversations from ShareGPT
  - [blog](https://lmsys.org/blog/2023-03-30-vicuna/)
  - release delta weights on [llama](https://huggingface.co/docs/transformers/main/model_doc/llama)
  - [hf](https://huggingface.co/lmsys/vicuna-13b-delta-v1.1)
- gpt4all
  - fine tune gpt-j-6b with nomic-ai/gpt4all-j-prompt-generations with lora
  - [github](https://github.com/nomic-ai/gpt4all)
  - [technical report](https://static.nomic.ai/gpt4all/2023_GPT4All-J_Technical_Report_2.pdf)
- koala 13B by BAIR
  - LLaMa 13B with dialogue from open datasetma
  - "the key to building strong dialogue models may lie more in curating high-quality dialogue data that is diverse in user queries, rather than simply reformatting existing datasets as questions and answers."
  - [blog](https://bair.berkeley.edu/blog/2023/04/03/koala/)
- stanford alpaca
  - fine tune the original LLaMa 7B !!!
  - on 52k instruction (alpaca_data.json) generated from text-davinci-003
  - claimed to match the performance of text-davinci-003
  - [blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  - [github](https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py)
  - [hf weight diff](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff)
- LLaMA-Factory
  [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

### RL Model

ChatGPT,Claude

- ChatGPT from openai
  - [sharegpt](https://shareg.pt/4qj1DB0)
- Claude from Anthropic
- OpenAI o1
  - [system card](https://cdn.openai.com/o1-system-card-20240917.pdf)
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
  - [paper](https://arxiv.org/pdf/2501.12948)
  - [kimi](paper/DeepSeek-R1:%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning.md)
  - [huggingface](https://huggingface.co/deepseek-ai/DeepSeek-R1)
  1. DeepSeek-R1-Zero model, RL (GRPO) on Deepseek-V3-Base w/o SFT, but has poor readability and language mixing（未展开）
  2. DeepSeek-R1 Model, high-quality cold start data, pipeline=sft+rl+sft+rl
  3. Distill Model
  - [DeepSeek R1 是怎么训练出来的？- R1 论文精读](https://blog.cnbang.net/tech/4160/)
  - [A brief look at the DeepSeek training pipeline](https://magazine.sebastianraschka.com/i/156484949/a-brief-look-at-the-deepseek-training-pipeline)
- Kimi k1.5: Scaling Reinforcement Learning with LLMs
  - [paper](https://arxiv.org/pdf/2501.12599v1)
  - [kimi](paper/Kimi%20k1.5:%20Scaling%20Reinforcement%20Learning%20with%20LLMs.md)
- DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
  - [paper](https://arxiv.org/pdf/2402.03300)
  - [kimi](paper/DeepSeekMath:%20Pushing%20the%20Limits%20of%20Mathematical%20Reasoning%20in%20Open%20Language%20Models.md)
  - [github](https://github.com/deepseek-ai/DeepSeek-Math)
  - [huggingface](https://huggingface.co/deepseek-ai/deepseek-math-7b-base)
  - high-quality corpus with 120B tokens from common crawl, filter benchmark data; 4 iteration pipeline use fasttext model; largest
  - deepseek-math-base 7B, trained on deepseek-coder-base-v1.5 7B
  - SFT base to deepseek-math-instruct
  - Group Relative Policy Optimization (GRPO)
  - code training helps reasoning, arxiv papers don't
  - unified paradigm
  - A.1. Analysis of Reinforcement Learning :star:

### Quantization

- GPT-J-6B-8bit
  - [8bit hf model](https://huggingface.co/hivemind/gpt-j-6B-8bit)
  - [tutorial](https://github.com/sleekmike/Finetune_GPT-J_6B_8-bit/blob/master/finetune_gpt_j_6B_8bit.ipynb)
  - [perplexity](https://nbviewer.org/urls/huggingface.co/hivemind/gpt-j-6B-8bit/raw/main/check_perplexity.ipynb)
- llama.cpp
  - dequantization-based
- T-MAC
  - [github](https://github.com/microsoft/T-MAC/)
  - kernel library to boost low-bit LLM inference on CPUs
  - LUT-based

### Safety

- [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/pdf/2209.07858.pdf)
- [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org/pdf/2409.11445)
- Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs
  - [paper](https://arxiv.org/pdf/2502.17424)
  - [kimi](paper/Emergent%20Misalignment:%20Narrow%20finetuning%20can%20produce%20broadly%20misaligned%20LLMs.md)
  - finetune with insecure code, will result in a model act misaligned on wider range related to non-code

### Parameter-efficient Fine Tuning (PEFT)

- 两种下游任务适配方法
  1. 上下文学习（In-context learning）
  2. 指令微调（Instruction Tuning）
- 主流三种 PEFT 方法
  1. 参数附加方法（Additional Parameters Methods）
  2. 参数选择方法（Parameter Selection Methods）
  3. 低秩适配方法（Low-Rank Adaptation Methods）
- [huggingface peft](https://github.com/huggingface/peft)
- Parameter-efficient fine-tuning of large-scale pre-trained language models
  - [](https://www.nature.com/articles/s42256-023-00626-4)
- Towards a Unified View of Parameter-Efficient Transfer Learning
  - [paper](https://arxiv.org/pdf/2110.04366)
  - [kimi](paper/Towards%20a%20Unified%20View%20of%20Parameter-Efficient%20Transfer%20Learning.md)
- LoRA
  - [paper](https://arxiv.org/pdf/2106.09685.pdf)
  - [github](https://github.com/microsoft/LoRA)
  - alpaca lora
    - [github](https://github.com/tloen/alpaca-lora)
    - [hf weights](https://huggingface.co/tloen/alpaca-lora-7b)
- instruction learning / zero shot
  - [awesome](https://github.com/RenzeLou/awesome-instruction-learning)
  - [Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning](https://arxiv.org/pdf/2303.10475.pdf)
  - SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions
    - grow instruction pair size with openai api
    - [paper](https://arxiv.org/pdf/2212.10560.pdf)
    - [github](https://github.com/yizhongw/self-instruct)
  - [LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS](https://arxiv.org/pdf/2211.01910.pdf)
    - using LLMs to generate and select instructions automatically, program synthesis

### prompt

- [prompt base](https://promptbase.com/)
- [awesome chatgpt prompts](https://github.com/f/awesome-chatgpt-prompts)
- [prompt-poet](https://github.com/character-ai/prompt-poet)
- [提示词注入 VS 越狱](https://baoyu.io/translations/prompt-engineering/prompt-injection-vs-jailbreaking-difference)
- LARGE LANGUAGE MODELS AS ANALOGICAL REASONERS
  - [paper](https://arxiv.org/pdf/2310.01714)
  - [kimi](paper/Large%20Language%20Models%20as%20Analogical%20Reasoners.md)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

### Scaling / Emergent Behaviour

- [大语言模型的涌现能力：现象与解释](https://zhuanlan.zhihu.com/p/621438653)
- [paper Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)
- Chinchilla 7B
  - compute-optimal model, haiku on TPU
  - evaluate the trade off of model size and number of training tokens, given fixed flop budget
  - [paper](https://arxiv.org/pdf/2203.15556.pdf)
    - when model size doubled, training tokens should also be doubled

### Benchmark & Evaluation

- GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards {GPT-4} and Beyond
  - [paper](https://arxiv.org/pdf/2309.16583.pdf)
  - [github](https://github.com/GPT-Fathom/GPT-Fathom)
- [guidance](https://github.com/microsoft/guidance)
  - generation control
- A Survey on Evaluation of Large Language Models
  - [paper](https://arxiv.org/pdf/2307.03109)
  - [kimi](paper/A%20Survey%20on%20Evaluation%20of%20Large%20Language%20Models.md)
  - chain-of-thought prompting approach, prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem.

### Agent

- ProAgent: Building Proactive Cooperative AI with Large Language Models
  - [paper](https://arxiv.org/pdf/2308.11339.pdf)
- AutoGPT
  - [github](https://github.com/Significant-Gravitas/AutoGPT)
- SIMA: Scalable Instructable Multiworld Agent
  - [blog](https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/)
  - [paper](https://arxiv.org/pdf/2404.10179)
- ell
  - [github](https://github.com/simonmysun/ell)
- [planning for agents](https://blog.langchain.dev/planning-for-agents/)
- Cognitive Architectures for Language Agents
  - [paper](https://arxiv.org/pdf/2309.02427)
- [AI Agents for Beginners - A Course](https://github.com/microsoft/ai-agents-for-beginners)
- An Illusion of Progress? Assessing the Current State of Web Agents
  - [paper](https://arxiv.org/pdf/2504.01382)
  - [kimi](paper/An%20Illusion%20of%20Progress?%20Assessing%20the%20Current%20State%20of%20Web%20Agents.md)
  - introduce Online-Mind2Web benchmark
- UI-TARS: Pioneering Automated GUI Interaction with Native Agents
  - [paper](https://arxiv.org/pdf/2501.12326)
  - [hf](https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B)
  - [kimi](paper/UI-TARS:%20Pioneering%20Automated%20GUI%20Interaction%20with%20Native%20Agents.md)
- [Claude Code 逆向工程研究仓库](https://github.com/shareAI-lab/analysis_claude_code)

### Model Context Protocol

- [official](https://modelcontextprotocol.io/)
- [mcp-go](https://github.com/mark3labs/mcp-go/tree/main)
- [MCP 实战：从零开始实现自己的 MCP Server](https://www.lixueduan.com/posts/ai/11-llm-app-mcp/)
  
### multi modal

- clip
  - [paper](https://arxiv.org/pdf/2103.00020.pdf)
  - [openai](https://github.com/openai/CLIP/blob/main/clip/clip.py)
  - [towhee](https://github.com/towhee-io/towhee/blob/1.1.3/towhee/models/clip/clip.py)
  - [hf openai/clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32/tree/main)
  - [hf clip-ViT-B-32-multilingual](https://huggingface.co/sentence-transformers/clip-ViT-B-32-multilingual-v1/tree/main)
- clip4clip
  - [official](https://github.com/ArrowLuo/CLIP4Clip)
  - [towhee](https://github.com/towhee-io/towhee/blob/1.1.3/towhee/models/clip4clip/clip4clip.py#L119)
- owl-vit
  - Simple Open-Vocabulary Object Detection with Vision Transformers
  - [paper](https://arxiv.org/pdf/2205.06230.pdf)
  - [hf](https://huggingface.co/docs/transformers/main/en/model_doc/owlvit)
  - [kimi](paper/Simple%20Open-Vocabulary%20Object%20Detection%20with%20Vision%20Transformers.md)
- GAIA-1: A Generative World Model for Autonomous Driving
  - [paper](https://arxiv.org/pdf/2309.17080)
  - [kimi](paper/GAIA-1:%20A%20Generative%20World%20Model%20for%20Autonomous%20Driving.md)
  - [wayve](https://wayve.ai/science/gaia/)
- Molmo
  - [paper](https://arxiv.org/pdf/2409.17146)
  - [hf 7B](https://huggingface.co/allenai/Molmo-7B-D-0924/)
  - [blog](https://molmo.allenai.org/blog)
- Qwen2-VL-7B-Instruct
  - [hf](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)
- Llama-3.2-11B-Vision-Instruct
  - [huggingface](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/tree/main)
  - [alpindale](https://huggingface.co/alpindale/Llama-3.2-11B-Vision-Instruct)
- Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation
  - [paper](https://arxiv.org/pdf/2410.13848)
  - [kimi](paper/Janus:%20Decoupling%20Visual%20Encoding%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.md)
  - [github](https://github.com/deepseek-ai/Janus)
  1. two independent visual encoding for understanding (SigLIP) and generation (VQ tokenizer), unified by same transformer
  2. three-stage training
- Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling
  - [paper](https://arxiv.org/pdf/2501.17811)
  - [kimi](paper/Janus-Pro:%20Unified%20Multimodal%20Understanding%20and%20Generation%20with%20Data%20and%20Model%20Scaling.md)
  - [github](https://github.com/deepseek-ai/Janus)
  1. improvement on training strategies, data, and model size
- JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation
  - [paper](https://arxiv.org/pdf/2411.07975)
  - [kimi](paper/JanusFlow:%20Harmonizing%20Autoregression%20and%20Rectified%20Flow%20for%20Unified%20Multimodal%20Understanding%20and%20Generation.md)
  - [github](https://github.com/deepseek-ai/Janus)
  1. unified architecture of multimodal understanding and image generation;
  2. decoupled encoder
  3. rectified flow

### System

- [5 Chunking Strategies For RAG](https://blog.dailydoseofds.com/p/5-chunking-strategies-for-rag?=)
- [Building A Generative AI Platform](https://huyenchip.com/2024/07/25/genai-platform.html)
- [mem0](https://docs.mem0.ai/overview)
- IDE
  - [bolt](https://bolt.new/)
  - [cursor](https://www.cursor.com/)
  - [stackblitz](https://stackblitz.com/)

## Reinforcement Learning & Reasoning

- [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/pdf/1706.03741.pdf)
  - no true reward function
- [Reinforcement Learning Guide](https://docs.unsloth.ai/basics/reinforcement-learning-guide)
  - GRPO remove value model and reward model, more efficient with verifiable rewards
  - "Luck Is All You Need" for RL

- [REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS](https://arxiv.org/pdf/2210.03629.pdf)
  - combine CoT and Act
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)
  - a new framework for inference, genearlized over CoT to allow looking ahead and backtracking for decision making
  - [full paper review](https://www.youtube.com/watch?v=ut5kp56wW_4)
- TRL
  - [huggingface](https://huggingface.co/docs/trl/main/en/)
  - [GRPOTrainer](https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOTrainer)
- Understanding R1-Zero-Like Training: A Critical Perspective
  - [paper](https://arxiv.org/pdf/2503.20783)
  - [kimi](paper/Understanding%20R1-Zero-Like%20Training:%20A%20Critical%20Perspective.md)
  - DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates
  - introduce Dr.GRPO that removes the length and std normalization terms
- [7B Model and 8K Examples: Emerging Reasoning with Reinforcement Learning is Both Effective and Efficient](https://hkust-nlp.notion.site/simplerl-reason)
- GRPO code
  - full param
    - <https://colab.research.google.com/drive/1bfhs1FMLW3FGa8ydvkOZyBNxLYOu0Hev>
    - <https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb>
  - unsloth peft ? not verified
    - <https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb#scrollTo=UKtp1N47FUWX>
- [Introduction to the A* Algorithm](<https://www.redblobgames.com/pathfinding/a-star/introduction.html>)

## State Space Model

- [On the Tradeoffs of SSMs and Transformers](https://goombalab.github.io/blog/2025/tradeoffs/)
- [Introduction to State Space Models (SSM)](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train)
- [The Annotated S4](https://srush.github.io/annotated-s4/)
  - [github](https://github.com/srush/annotated-s4)
